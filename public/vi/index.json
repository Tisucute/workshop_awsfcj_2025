[
{
	"uri": "//localhost:1313/vi/",
	"title": "Xây dựng kiến trúc xử lý tự động và trực quan hóa dữ liệu thời gian thực trên AWS",
	"tags": [],
	"description": "",
	"content": "Xây dựng kiến trúc xử lý tự động và trực quan hóa dữ liệu thời gian thực trên AWS Họ tên: Nguyễn Ngọc Anh Thư\nMSSV: 22133059\nEmail: ngocanhthugialai@gmail.com\nTrường: Đại học Sư phạm Kỹ thuật TP.HCM (HCMUTE)\nTổng quan Đề tài \u0026ldquo;Xây dựng kiến trúc xử lý tự động và trực quan hóa dữ liệu thời gian thực trên AWS\u0026rdquo; là sự tâm huyết và nỗ lực nhằm xây dựng một hệ thống toàn diện, tự động hóa việc thu thập, xử lý và hiển thị trực quan các luồng dữ liệu thời gian thực dựa trên nền tảng Amazon Web Services (AWS). Với mục tiêu đáp ứng nhu cầu phân tích và ra quyết định nhanh chóng, hiệu quả dựa trên dữ liệu mới nhất, hệ thống được thiết kế bao gồm các thành phần chính: thu thập dữ liệu thời tiết từ API của OpenWeather bằng Python và lưu trữ vào AWS S3; orchestration thông qua Amazon Managed Workflows for Apache Airflow (MWAA); xử lý và chuyển đổi dữ liệu bằng AWS Glue; lưu trữ dữ liệu vào kho dữ liệu AWS Redshift; và cuối cùng trực quan hóa kết quả bằng AWS QuickSight trên dashboard trực quan.\nViệc triển khai hạ tầng được thực hiện thông qua mã nguồn (Infrastructure as Code - IaC) sử dụng AWS CloudFormation, đảm bảo tính nhất quán, tự động hóa và dễ dàng quản lý, mở rộng khi có nhu cầu thay đổi hoặc bổ sung tài nguyên. Các chính sách bảo mật và quản lý truy cập cũng được thiết lập thông qua AWS IAM và môi trường bảo mật mạng riêng ảo (VPC) nhằm đảm bảo an toàn cho hệ thống.\nEm rất hy vọng báo cáo này không chỉ phản ánh chân thực quá trình thực hiện của em, mà còn là một tài liệu thân thiện, dễ hiểu, đồng hành cùng các bạn sinh viên mới tiếp cận AWS hoặc các mô hình phát triển hiện đại. Mong rằng, qua đây em có thể truyền tải được sự nhiệt huyết, niềm đam mê với công nghệ và hỗ trợ các bạn tự tin hơn trên hành trình khám phá và ứng dụng AWS.\nThời gian thực hiện Lấy dữ liệu: 1 ngày Dữ liệu chỉ đủ để phân tích, nếu nhiều hơn có thể phát sinh thêm nhiều chi phí\nThực hiện các thao tác: 2 - 3 tiếng (tùy vào độ quen thuộc với các công cụ) Nội dung Giới thiệu Các công cụ sử dụng và Sơ đồ kiến trúc Các công cụ sử dụng Sơ đồ kiến trúc Các bước thực hiện quá trình ETL tự động Về các file môi trường Triển khai hạ tầng bằng mã nguồn Thiết lập biến, tạo kết nối và upload môi trường Quá trình thực hiện ETL Phân tích và trực quan hóa Phân tích với AWS Redshift Trực quan hóa với AWS Quicksight Dọn dẹp tài nguyên Các lỗi có thể gặp và cách khắc phục Kết luận "
},
{
	"uri": "//localhost:1313/vi/3-etl-process/3.1-env-files/",
	"title": "Các file môi trường",
	"tags": [],
	"description": "",
	"content": "Trong hệ thống ETL tự động trên AWS, các file môi trường là tập hợp mã nguồn Python và cấu hình được sử dụng bởi Apache Airflow (MWAA) để điều phối toàn bộ quy trình thu thập – xử lý – lưu trữ dữ liệu. Các file này được lưu trữ trong Amazon S3, và được phân chia rõ ràng theo vai trò: môi trường của Airflow và môi trường thực thi của Glue.\nVề môi trường của Airflow 1. openweather_api.py – DAG thu thập dữ liệu Đây là file đầu tiên cần được triển khai trong môi trường Airflow. File này định nghĩa một DAG tên là openweather_api_dag, có nhiệm vụ:\nGọi OpenWeather API để lấy dữ liệu thời tiết thời gian thực tại TP. Hồ Chí Minh.\nXử lý dữ liệu JSON trả về từ API thành định dạng bảng (dạng DataFrame của pandas).\nChuyển dữ liệu sang chuỗi CSV.\nUpload file CSV này lên Amazon S3 (bucket vùng raw) để phục vụ bước ETL tiếp theo.\nĐiểm nổi bật:\nSử dụng requests để gọi API, pandas để xử lý dữ liệu.\nAPI key được bảo mật bằng cách lấy từ Airflow Variable (Variable.get(\u0026lsquo;key\u0026rsquo;)).\nGiao tiếp giữa các task trong DAG thông qua XCom.\nUpload file bằng S3CreateObjectOperatoropenweather_api.\nDAG này được thiết lập để chạy mỗi giờ, đảm bảo dữ liệu luôn được cập nhật liên tục.\n2. transform_redshift_load.py – DAG điều phối quá trình ETL DAG này tên là transform_redshift_dag, dùng để:\nChờ DAG openweather_api_dag chạy thành công, thông qua ExternalTaskSensor.\nSau đó gọi AWS Glue Job để thực hiện bước ETL: chuyển đổi dữ liệu và nạp vào Redshift.\nDAG sử dụng GlueJobOperator để gọi script ETL transform.py nằm ở ngoài môi trường Airflow (trong thư mục S3 dành riêng cho Glue)transform_redshift_load.\n3. requirements.txt Nếu cần cài thêm thư viện cho Airflow (như numpy, pandas, requests), file này sẽ liệt kê các package cần cài. Được upload lên S3 tại thư mục cấu hình requirements/ để MWAA đọc khi khởi tạo môi trường.\n4. transform.py – Script ETL chạy trong AWS Glue File này không nằm trong môi trường Airflow, mà được AWS Glue sử dụng để xử lý dữ liệu bằng PySpark:\nĐọc file CSV từ S3 (được tạo bởi DAG đầu tiên).\nChuyển đổi kiểu dữ liệu, chuẩn hóa timestamp (dt, dt_iso, ingest_time).\nGhi dữ liệu vào Redshift, đồng thời tạo bảng nếu chưa tồn tại (CREATE TABLE IF NOT EXISTS \u0026hellip;)transform.\nFile này được gọi trong DAG transform_redshift_dag thông qua script_location.\n"
},
{
	"uri": "//localhost:1313/vi/1-introduce/",
	"title": "Giới thiệu",
	"tags": [],
	"description": "",
	"content": "\rNguyễn Ngọc Anh Thư – 22133059\rTrường Đại học Sư phạm Kỹ thuật TP. Hồ Chí Minh\rXây dựng kiến trúc xử lý tự động và trực quan hóa dữ liệu thời gian thực trên AWS\rBáo cáo này trình bày quá trình xây dựng một hệ thống xử lý và trực quan hóa dữ liệu thời gian thực trên AWS, bao gồm các nội dung chính sau:\rGiới thiệu: Sơ lược về các nội dung liên quan tới đề tài.\rCác công cụ sử dụng \u0026 Sơ đồ kiến trúc: Trình bày các dịch vụ AWS, công cụ hỗ trợ và sơ đồ tổng thể của hệ thống.\rCác bước thực hiện quá trình ETL tự động: Hướng dẫn chi tiết từng bước từ chuẩn bị môi trường, triển khai hạ tầng, thiết lập kết nối đến thực thi ETL.\rPhân tích \u0026 trực quan hóa: Mô tả cách phân tích dữ liệu với AWS Redshift và xây dựng dashboard trực quan với AWS QuickSight.\rDọn dẹp tài nguyên: Hướng dẫn xóa các tài nguyên AWS sau khi hoàn thành để tối ưu chi phí.\rThời gian thực hiện\rLấy dữ liệu: 1 ngày\rThực hiện các thao tác: 2 - 3 tiếng (tùy vào độ quen thuộc với các công cụ)\rChú ý: Dữ liệu chỉ đủ để phân tích, nếu nhiều hơn có thể phát sinh thêm nhiều chi phí.\rĐiều kiện tiên quyết\rCó tài khoản AWS và quyền truy cập các dịch vụ cần thiết (S3, IAM, Glue, Redshift, MWAA, QuickSight,...).\rĐã cài đặt Python và các thư viện liên quan (requests, boto3,...).\rMáy tính kết nối internet ổn định.\rKiến thức cơ bản về AWS và dòng lệnh.\rKhoản phí ước tính:Trong khoảng từ 20$ tới 30$\r"
},
{
	"uri": "//localhost:1313/vi/2-architecture/2.1-tools/",
	"title": "Các công cụ được sử dụng",
	"tags": [],
	"description": "",
	"content": "1. OpenWeather API OpenWeather API là dịch vụ cung cấp dữ liệu thời tiết thời gian thực thông qua các RESTful endpoint. Người dùng có thể truy xuất các thông tin như nhiệt độ, độ ẩm, áp suất, v.v. bằng cách gửi yêu cầu HTTP và nhận về dữ liệu ở định dạng JSON hoặc XML.\n2. Python Script Python là ngôn ngữ lập trình phổ biến, mạnh mẽ trong xử lý dữ liệu. Trong hệ thống này, Python script được sử dụng để:\nGọi API (ví dụ: OpenWeather API) để lấy dữ liệu.\nPhân tích cú pháp (parse) dữ liệu JSON trả về.\nLàm sạch, chuẩn hóa dữ liệu (chuyển đổi múi giờ, đổi tên trường, v.v.).\nXuất dữ liệu ra các định dạng như CSV, Parquet, hoặc JSON.\nĐẩy dữ liệu lên Amazon S3 để lưu trữ.\n3. Amazon S3 Amazon S3 là dịch vụ lưu trữ đối tượng (object storage) của AWS, cho phép lưu trữ và truy xuất dữ liệu với độ bền và khả năng mở rộng cao. Trong kiến trúc này, S3 được dùng để:\nLưu trữ dữ liệu thô (raw data) do Python script tạo ra.\nLưu trữ các file định nghĩa DAGs cho Airflow.\nLưu trữ các file requirements.txt để quản lý dependencies cho Airflow.\n4. AWS Glue AWS Glue là dịch vụ ETL (Extract, Transform, Load) serverless của AWS, hỗ trợ tự động hóa việc phát hiện schema, làm sạch, biến đổi và nạp dữ liệu. Các thành phần chính:\nGlue Crawler: Tự động quét dữ liệu trên S3, phát hiện schema và tạo metadata table trong Data Catalog.\nGlue ETL Job: Chạy mã Scala hoặc Python (Spark) để xử lý, biến đổi dữ liệu và xuất kết quả sang S3 hoặc Redshift.\n5. Amazon Redshift Amazon Redshift là dịch vụ data warehouse trên nền tảng đám mây, tối ưu cho các truy vấn phân tích dữ liệu lớn (OLAP). Redshift hỗ trợ lưu trữ dữ liệu theo mô hình star/snowflake schema, cho phép thực hiện các truy vấn phức tạp với hiệu năng cao, phù hợp cho các bài toán BI, phân tích dữ liệu.\n6. Apache Airflow (MWAA) Apache Airflow là nền tảng mã nguồn mở dùng để lập lịch, điều phối và giám sát các workflow (luồng công việc) phức tạp. Trên AWS, Airflow được cung cấp dưới dạng dịch vụ Managed Workflows for Apache Airflow (MWAA). Airflow sử dụng các DAGs (Directed Acyclic Graphs) để định nghĩa các bước xử lý dữ liệu, hỗ trợ tự động hóa, retry, alert, và tích hợp với nhiều dịch vụ AWS.\n7. Amazon QuickSight Amazon QuickSight là dịch vụ BI (Business Intelligence) trên AWS, hỗ trợ kết nối đến nhiều nguồn dữ liệu như Redshift, S3,\u0026hellip; QuickSight cho phép xây dựng dashboard, biểu đồ, báo cáo tương tác với giao diện kéo-thả, hỗ trợ phân tích và trực quan hóa dữ liệu cho người dùng doanh nghiệp.\n8. AWS CloudFormation, IAM, VPC, Secrets Manager CloudFormation: Dịch vụ quản lý hạ tầng dưới dạng mã (Infrastructure as Code), giúp tự động hóa việc triển khai và quản lý tài nguyên AWS.\nIAM (Identity and Access Management): Quản lý quyền truy cập, xác thực và phân quyền cho người dùng/dịch vụ.\nVPC (Virtual Private Cloud): Tạo mạng ảo riêng biệt trên AWS, kiểm soát truy cập và bảo mật cho các tài nguyên.\nSecrets Manager: Lưu trữ, quản lý và truy xuất thông tin nhạy cảm (API key, mật khẩu, v.v.) một cách an toàn.\n"
},
{
	"uri": "//localhost:1313/vi/2-architecture/",
	"title": "Các công cụ sử dụng và Sơ đồ kiến trúc",
	"tags": [],
	"description": "",
	"content": "Hệ thống ETL thời tiết được xây dựng trên nền tảng AWS, kết hợp với các công cụ mã nguồn mở như Python và Apache Airflow. Dữ liệu thời tiết được lấy từ OpenWeather API, xử lý bởi script Python và điều phối bằng Airflow (MWAA). Dữ liệu thô được lưu tạm vào Amazon S3, sau đó được AWS Glue xử lý và chuẩn hóa trước khi nạp vào kho dữ liệu Amazon Redshift. Cuối cùng, Amazon QuickSight được dùng để trực quan hóa dữ liệu phục vụ phân tích. Toàn bộ hạ tầng được triển khai tự động qua CloudFormation và bảo mật bởi IAM, VPC, và Secrets Manager. Sơ đồ kiến trúc thể hiện rõ luồng dữ liệu và vai trò của từng thành phần trong hệ thống.\nNội dung chính: 2.1 Các công cụ sử dụng\n2.2 Sơ đồ kiến trúc\n"
},
{
	"uri": "//localhost:1313/vi/2-architecture/2.2-architecture/",
	"title": "Xây dựng Sơ đồ Kiến trúc",
	"tags": [],
	"description": "",
	"content": "Sơ đồ Kiến trúc Kiến trúc này mô tả một quy trình thu thập, xử lý, lưu trữ, phân tích và trực quan hóa dữ liệu thời tiết thông qua các dịch vụ AWS và công cụ mã nguồn mở. Ban đầu, người dùng gửi yêu cầu thông tin thời tiết. Một Python script được đóng gói và lưu trữ vào môi trường Amazon S3, để Apache Airflow (MWAA) tự động gọi thực thi theo lịch định sẵn. Python script này sử dụng thư viện PySpark (Spark được tích hợp sẵn trong script Python) để gọi API của OpenWeather, lấy dữ liệu thời tiết theo thời gian thực, xử lý sơ bộ và lưu vào bucket Amazon S3 dưới dạng dữ liệu thô. Tiếp theo, AWS Glue Job tiến hành các bước ETL nâng cao, làm sạch, chuẩn hóa dữ liệu và nạp vào Amazon Redshift, sẵn sàng cho việc phân tích chuyên sâu. Sau đó, Amazon QuickSight kết nối với Redshift để tạo ra dashboard tương tác, trực quan hóa dữ liệu giúp người dùng dễ dàng phân tích và ra quyết định.\nToàn bộ hạ tầng và các dịch vụ này được quản lý và triển khai tự động bằng AWS CloudFormation, đồng thời được bảo mật chặt chẽ bởi AWS IAM, VPC và Secrets Manager, giúp kiểm soát truy cập và lưu trữ an toàn các thông tin nhạy cảm trong hệ thống.\nSơ đồ dưới đây minh họa tổng thể kiến trúc hệ thống trên AWS: "
},
{
	"uri": "//localhost:1313/vi/3-etl-process/",
	"title": "Các bước thực hiện quá trình ETL tự động",
	"tags": [],
	"description": "",
	"content": "Trong kiến trúc thu thập và xử lý dữ liệu thời tiết sử dụng AWS, phần quan trọng nhất nằm ở quy trình ETL (Extract – Transform – Load) tự động. Đây là nơi toàn bộ dữ liệu được lấy về từ OpenWeather API, xử lý, lưu trữ và chuyển hóa thành thông tin có thể phân tích được.\nLuồng xử lý tổng thể gồm 4 bước chính: 1. Chuẩn bị môi trường Viết script Python để gọi API và xử lý dữ liệu ban đầu.\nĐóng gói các file này vào S3 để Airflow (MWAA) có thể chạy tự động.\n2. Triển khai hạ tầng tự động Sử dụng AWS CloudFormation để triển khai toàn bộ tài nguyên cần thiết như S3, MWAA, IAM, Glue, Redshift\u0026hellip;\n3. Cấu hình Airflow Thiết lập biến (Variables) và kết nối (Connections) trong Airflow.\nUpload DAG và các tệp cần thiết lên đúng vị trí trong S3.\n4. Thực thi ETL Apache Airflow tự động kích hoạt pipeline theo lịch trình.\nGọi API → lưu vào S3 → xử lý bằng Glue → lưu vào Redshift → trực quan hóa bằng QuickSight.\nTại sao cần ETL tự động? Tiết kiệm thời gian: Không cần thủ công tải dữ liệu hằng ngày.\nChuẩn hóa dữ liệu: Đảm bảo dữ liệu có định dạng sạch, đồng nhất, có thể phân tích.\nTích hợp dễ dàng: Với các công cụ phân tích như Amazon Redshift \u0026amp; QuickSight.\nCó thể mở rộng: Dễ dàng áp dụng cho nhiều loại dữ liệu khác nhau, không chỉ thời tiết.\nNội dung chính: 3.1 Về các file môi trường\n3.2 Triển khai hạ tầng bằng mã nguồn\n3.3 Thiết lập biến, tạo kết nối và upload môi trường\n3.4 Quá trình thực hiện ETL\n"
},
{
	"uri": "//localhost:1313/vi/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "//localhost:1313/vi/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]