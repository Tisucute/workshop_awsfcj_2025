[
{
	"uri": "//localhost:1313/vi/",
	"title": "Xây dựng kiến trúc xử lý tự động và trực quan hóa dữ liệu thời gian thực trên AWS",
	"tags": [],
	"description": "",
	"content": "Xây dựng kiến trúc xử lý tự động và trực quan hóa dữ liệu thời gian thực trên AWS Họ tên: Nguyễn Ngọc Anh Thư\nMSSV: 22133059\nEmail: ngocanhthugialai@gmail.com\nTrường: Đại học Sư phạm Kỹ thuật TP.HCM (HCMUTE)\nTổng quan Đề tài \u0026ldquo;Xây dựng kiến trúc xử lý tự động và trực quan hóa dữ liệu thời gian thực trên AWS\u0026rdquo; là sự tâm huyết và nỗ lực nhằm xây dựng một hệ thống toàn diện, tự động hóa việc thu thập, xử lý và hiển thị trực quan các luồng dữ liệu thời gian thực dựa trên nền tảng Amazon Web Services (AWS). Với mục tiêu đáp ứng nhu cầu phân tích và ra quyết định nhanh chóng, hiệu quả dựa trên dữ liệu mới nhất, hệ thống được thiết kế bao gồm các thành phần chính: thu thập dữ liệu thời tiết từ API của OpenWeather bằng Python và lưu trữ vào AWS S3; orchestration thông qua Amazon Managed Workflows for Apache Airflow (MWAA); xử lý và chuyển đổi dữ liệu bằng AWS Glue; lưu trữ dữ liệu vào kho dữ liệu AWS Redshift; và cuối cùng trực quan hóa kết quả bằng AWS QuickSight trên dashboard trực quan.\nViệc triển khai hạ tầng được thực hiện thông qua mã nguồn (Infrastructure as Code - IaC) sử dụng AWS CloudFormation, đảm bảo tính nhất quán, tự động hóa và dễ dàng quản lý, mở rộng khi có nhu cầu thay đổi hoặc bổ sung tài nguyên. Các chính sách bảo mật và quản lý truy cập cũng được thiết lập thông qua AWS IAM và môi trường bảo mật mạng riêng ảo (VPC) nhằm đảm bảo an toàn cho hệ thống.\nEm rất hy vọng báo cáo này không chỉ phản ánh chân thực quá trình thực hiện của em, mà còn là một tài liệu thân thiện, dễ hiểu, đồng hành cùng các bạn sinh viên mới tiếp cận AWS hoặc các mô hình phát triển hiện đại. Mong rằng, qua đây em có thể truyền tải được sự nhiệt huyết, niềm đam mê với công nghệ và hỗ trợ các bạn tự tin hơn trên hành trình khám phá và ứng dụng AWS.\nThời gian thực hiện Lấy dữ liệu: 1 ngày Dữ liệu chỉ đủ để phân tích, nếu nhiều hơn có thể phát sinh thêm nhiều chi phí\nThực hiện các thao tác: 2 - 3 tiếng (tùy vào độ quen thuộc với các công cụ) Nội dung Giới thiệu Các công cụ sử dụng và Sơ đồ kiến trúc Các công cụ sử dụng Sơ đồ kiến trúc Các bước thực hiện quá trình ETL tự động Về các file môi trường Triển khai hạ tầng bằng mã nguồn Thiết lập biến, tạo kết nối và upload môi trường Quá trình thực hiện ETL Phân tích và trực quan hóa Phân tích với AWS Redshift Trực quan hóa với AWS Quicksight Dọn dẹp tài nguyên "
},
{
	"uri": "//localhost:1313/vi/1-introduce/",
	"title": "Giới thiệu",
	"tags": [],
	"description": "",
	"content": "\rNguyễn Ngọc Anh Thư – 22133059\rTrường Đại học Sư phạm Kỹ thuật TP. Hồ Chí Minh\rXây dựng kiến trúc xử lý tự động và trực quan hóa dữ liệu thời gian thực trên AWS\rBáo cáo này trình bày quá trình xây dựng một hệ thống xử lý và trực quan hóa dữ liệu thời gian thực trên AWS, bao gồm các nội dung chính sau:\rGiới thiệu: Sơ lược về các nội dung liên quan tới đề tài.\rCác công cụ sử dụng \u0026 Sơ đồ kiến trúc: Trình bày các dịch vụ AWS, công cụ hỗ trợ và sơ đồ tổng thể của hệ thống.\rCác bước thực hiện quá trình ETL tự động: Hướng dẫn chi tiết từng bước từ chuẩn bị môi trường, triển khai hạ tầng, thiết lập kết nối đến thực thi ETL.\rPhân tích \u0026 trực quan hóa: Mô tả cách phân tích dữ liệu với AWS Redshift và xây dựng dashboard trực quan với AWS QuickSight.\rDọn dẹp tài nguyên: Hướng dẫn xóa các tài nguyên AWS sau khi hoàn thành để tối ưu chi phí.\rThời gian thực hiện\rLấy dữ liệu: 1 ngày\rThực hiện các thao tác: 2 - 3 tiếng (tùy vào độ quen thuộc với các công cụ)\rChú ý: Dữ liệu chỉ đủ để phân tích, nếu nhiều hơn có thể phát sinh thêm nhiều chi phí.\r"
},
{
	"uri": "//localhost:1313/vi/2-architecture/2.1-tools/",
	"title": "Các công cụ được sử dụng",
	"tags": [],
	"description": "",
	"content": "Các công cụ được sử dụng 1. Data Ingestion OpenWeather API: Nguồn dữ liệu thời tiết thời gian thực (temperature, humidity, v.v.) qua RESTful endpoint. Python Script: Gọi API, parse JSON, làm sạch sơ bộ (ví dụ: chuyển timezone, chuẩn hoá tên trường…) Xuất file (CSV/Parquet/JSON) và ghi lên Amazon S3. 2. Staging \u0026amp; Storage Amazon S3 Bucket: Raw zone: chứa file thô do Python script tạo. /dags: nơi lưu definition của Airflow DAGs. requirements.txt: liệt kê dependencies (requests, boto3, pandas…) để Airflow tự động cài trước khi chạy task. 3. Transformation AWS Glue: Glue Crawler (tuỳ chọn): tự động phát hiện schema từ file raw trên S3 và tạo table metadata trong Data Catalog. Glue ETL Job: chạy Scala/Python Spark để: Đọc dữ liệu từ S3 (raw) Làm sạch/biến đổi (ví dụ: filter records, join thêm lookup table, enrichment…) Xuất dữ liệu đã transform sang S3 clean zone hoặc trực tiếp load vào Redshift. 4. Data Warehouse Amazon Redshift Cluster: Lưu trữ bảng đã qua xử lý (dim, fact) theo mô hình star/snowflake schema. Hỗ trợ query phân tích khối lượng lớn với hiệu năng cao. 5. Orchestration Apache Airflow: Scheduler \u0026amp; Web UI: lên lịch (cron, interval), trigger manual, monitoring, retry, alert khi có lỗi. DAGs (viết bằng Python): định nghĩa luồng công việc. PythonOperator: chạy script ingestion. GlueJobOperator: khởi job AWS Glue. RedshiftOperator hoặc PostgresOperator: chạy SQL trên Redshift nếu cần. Triển khai code lên Airflow bằng cách mount folder /dags từ S3 và requirements.txt để quản lý dependencies. 6. Analyze AWS Athena: Cho phép query trực tiếp file dữ liệu trên S3 (clean zone) bằng SQL mà không cần load vào database. Amazon QuickSight: Kết nối đến Redshift (hoặc Athena) để dựng dashboard, chart, alert; cung cấp giao diện drag-and-drop cho business – Gọi API, parse JSON, làm sạch sơ bộ (ví dụ: chuyển timezone, chuẩn hoá tên trường…) – Xuất file (CSV/Parquet/JSON) và ghi lên Amazon S3. "
},
{
	"uri": "//localhost:1313/vi/2-architecture/",
	"title": "Kiến trúc",
	"tags": [],
	"description": "",
	"content": "Kiến trúc "
},
{
	"uri": "//localhost:1313/vi/2-architecture/2.2-architecture/",
	"title": "Xây dựng Sơ đồ Kiến trúc",
	"tags": [],
	"description": "",
	"content": "Sơ đồ Kiến trúc Pipeline trên AWS Cloud tự động thu thập dữ liệu từ OpenWeather API bằng Python và lưu vào S3, sau đó AWS Glue làm sạch và chuyển đổi dữ liệu trước khi nạp vào Redshift theo star-schema; Amazon Athena và QuickSight phục vụ phân tích ad-hoc và trực quan hóa, còn Apache Airflow điều phối toàn bộ quy trình với retry, logging và khả năng mở rộng linh hoạt. Sơ đồ dưới đây minh họa tổng thể kiến trúc hệ thống trên AWS: "
},
{
	"uri": "//localhost:1313/vi/3-data/",
	"title": "Tập dữ liệu được sử dụng",
	"tags": [],
	"description": "",
	"content": "Giới thiệu về tập dữ liệu lấy từ OpenWeather API Tập dữ liệu thu thập từ OpenWeather API là một chuỗi thời gian (time-series) các bản ghi thời tiết, mỗi bản ghi tương ứng với một thời điểm quan sát tại một vị trí địa lý cụ thể. Bộ dữ liệu này rất phù hợp cho các bài toán phân tích biến động khí hậu, dự báo thời tiết, trực quan hóa dashboard hoặc xây dựng mô hình máy học.\nCác trường dữ liệu chính gồm:\nThông tin chung:\ndt: Thời điểm quan sát (UNIX timestamp) timezone: Độ lệch múi giờ so với UTC id, name: Mã và tên thành phố Vị trí địa lý:\ncoord.lat, coord.lon: Vĩ độ, kinh độ Thông số thời tiết chính (main):\ntemp: Nhiệt độ trung bình feels_like: Cảm giác nhiệt độ temp_min, temp_max: Nhiệt độ cực tiểu/cực đại pressure: Áp suất khí quyển humidity: Độ ẩm Thời tiết chi tiết (weather - mảng):\nmain: Mô tả ngắn (ví dụ: \u0026ldquo;Clear\u0026rdquo;, \u0026ldquo;Rain\u0026rdquo;) description: Mô tả chi tiết hơn icon: Mã icon hiển thị Gió:\nwind.speed: Tốc độ gió (m/s) wind.deg: Hướng gió (độ) Mây:\nclouds.all: Phần trăm che phủ mây Tầm nhìn:\nvisibility: Tầm nhìn (mét) Lượng mưa/tuyết (nếu có):\nrain.1h, rain.3h: Lượng mưa trong 1h/3h snow.1h, snow.3h: Lượng tuyết trong 1h/3h Thời điểm mặt trời mọc/lặn:\nsys.sunrise, sys.sunset: Thời điểm mặt trời mọc/lặn (UNIX timestamp) Lưu ý:\nDữ liệu thường được thu thập định kỳ (ví dụ mỗi 15–60 phút), lưu dưới dạng JSON gốc hoặc chuyển thành bảng CSV với các\n"
},
{
	"uri": "//localhost:1313/vi/6-cleanup/",
	"title": "Dọn dẹp tài nguyên  ",
	"tags": [],
	"description": "",
	"content": "Chúng ta sẽ tiến hành các bước sau để xóa các tài nguyên chúng ta đã tạo trong bài thực hành này.\nXóa EC2 instance Truy cập giao diện quản trị dịch vụ EC2 Click Instances. Click chọn cả 2 instance Public Linux Instance và Private Windows Instance. Click Instance state. Click Terminate instance, sau đó click Terminate để xác nhận. Truy cập giao diện quản trị dịch vụ IAM Click Roles. Tại ô tìm kiếm , điền SSM. Click chọn SSM-Role. Click Delete, sau đó điền tên role SSM-Role và click Delete để xóa role. Click Users. Click chọn user Portfwd. Click Delete, sau đó điền tên user Portfwd và click Delete để xóa user. Xóa S3 bucket Truy cập giao diện quản trị dịch vụ System Manager - Session Manager. Click tab Preferences. Click Edit. Kéo chuột xuống dưới. Tại mục S3 logging. Bỏ chọn Enable để tắt tính năng logging. Kéo chuột xuống dưới. Click Save. Truy cập giao diện quản trị dịch vụ S3 Click chọn S3 bucket chúng ta đã tạo cho bài thực hành. ( Ví dụ : lab-fcj-bucket-0001 ) Click Empty. Điền permanently delete, sau đó click Empty để tiến hành xóa object trong bucket. Click Exit. Sau khi xóa hết object trong bucket, click Delete Điền tên S3 bucket, sau đó click Delete bucket để tiến hành xóa S3 bucket. Xóa các VPC Endpoint Truy cập vào giao diện quản trị dịch vụ VPC Click Endpoints. Chọn 4 endpoints chúng ta đã tạo cho bài thực hành bao gồm SSM, SSMMESSAGES, EC2MESSAGES, S3GW. Click Actions. Click Delete VPC endpoints. Tại ô confirm , điền delete. Click Delete để tiến hành xóa các endpoints. Click biểu tượng refresh, kiểm tra tất cả các endpoints đã bị xóa trước khi làm bước tiếp theo. Xóa VPC Truy cập vào giao diện quản trị dịch vụ VPC Click Your VPCs. Click chọn Lab VPC. Click Actions. Click Delete VPC. Tại ô confirm, điền delete để xác nhận, click Delete để thực hiện xóa Lab VPC và các tài nguyên liên quan. "
},
{
	"uri": "//localhost:1313/vi/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "//localhost:1313/vi/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]