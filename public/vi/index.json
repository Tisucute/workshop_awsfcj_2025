[
{
	"uri": "//localhost:1313/vi/",
	"title": "Xây dựng kiến trúc xử lý tự động và trực quan hóa dữ liệu thời gian thực trên AWS",
	"tags": [],
	"description": "",
	"content": "Xây dựng kiến trúc xử lý tự động và trực quan hóa dữ liệu thời gian thực trên AWS Họ tên: Nguyễn Ngọc Anh Thư\nMSSV: 22133059\nEmail: ngocanhthugialai@gmail.com\nTrường: Đại học Sư phạm Kỹ thuật TP.HCM (HCMUTE)\nTổng quan Đề tài \u0026ldquo;Xây dựng kiến trúc xử lý tự động và trực quan hóa dữ liệu thời gian thực trên AWS\u0026rdquo; là sự tâm huyết và nỗ lực nhằm xây dựng một hệ thống toàn diện, tự động hóa việc thu thập, xử lý và hiển thị trực quan các luồng dữ liệu thời gian thực dựa trên nền tảng Amazon Web Services (AWS). Với mục tiêu đáp ứng nhu cầu phân tích và ra quyết định nhanh chóng, hiệu quả dựa trên dữ liệu mới nhất, hệ thống được thiết kế bao gồm các thành phần chính: thu thập dữ liệu thời tiết từ API của OpenWeather bằng Python và lưu trữ vào AWS S3; orchestration thông qua Amazon Managed Workflows for Apache Airflow (MWAA); xử lý và chuyển đổi dữ liệu bằng AWS Glue; lưu trữ dữ liệu vào kho dữ liệu AWS Redshift; và cuối cùng trực quan hóa kết quả bằng AWS QuickSight trên dashboard trực quan.\nViệc triển khai hạ tầng được thực hiện thông qua mã nguồn (Infrastructure as Code - IaC) sử dụng AWS CloudFormation, đảm bảo tính nhất quán, tự động hóa và dễ dàng quản lý, mở rộng khi có nhu cầu thay đổi hoặc bổ sung tài nguyên. Các chính sách bảo mật và quản lý truy cập cũng được thiết lập thông qua AWS IAM và môi trường bảo mật mạng riêng ảo (VPC) nhằm đảm bảo an toàn cho hệ thống.\nEm rất hy vọng báo cáo này không chỉ phản ánh chân thực quá trình thực hiện của em, mà còn là một tài liệu thân thiện, dễ hiểu, đồng hành cùng các bạn sinh viên mới tiếp cận AWS hoặc các mô hình phát triển hiện đại. Mong rằng, qua đây em có thể truyền tải được sự nhiệt huyết, niềm đam mê với công nghệ và hỗ trợ các bạn tự tin hơn trên hành trình khám phá và ứng dụng AWS.\nThời gian thực hiện Lấy dữ liệu: 1 ngày Dữ liệu chỉ đủ để phân tích, nếu nhiều hơn có thể phát sinh thêm nhiều chi phí\nThực hiện các thao tác: 2 - 3 tiếng (tùy vào độ quen thuộc với các công cụ) Nội dung Giới thiệu Các công cụ sử dụng và Sơ đồ kiến trúc Các công cụ sử dụng Sơ đồ kiến trúc Các bước thực hiện quá trình ETL tự động Về các file môi trường Triển khai hạ tầng bằng mã nguồn Thiết lập biến, tạo kết nối và upload môi trường Quá trình thực hiện ETL Phân tích và trực quan hóa Phân tích với AWS Redshift Trực quan hóa với AWS Quicksight Dọn dẹp tài nguyên Kết luận "
},
{
	"uri": "//localhost:1313/vi/3-etl-process/3.1-env-files/",
	"title": "Các file môi trường",
	"tags": [],
	"description": "",
	"content": "Trong hệ thống ETL tự động trên AWS, các file môi trường là tập hợp mã nguồn Python và cấu hình được sử dụng bởi Apache Airflow (MWAA) để điều phối toàn bộ quy trình thu thập – xử lý – lưu trữ dữ liệu. Các file này được lưu trữ trong Amazon S3, và được phân chia rõ ràng theo vai trò: môi trường của Airflow và môi trường thực thi của AWS Glue.\nVề môi trường của Airflow 1. openweather_api.py – DAG thu thập dữ liệu thời tiết Đây là file đầu tiên cần được triển khai trong môi trường Airflow. File này định nghĩa một DAG tên là openweather_api_dag, có nhiệm vụ:\nGọi OpenWeather API để lấy dữ liệu thời tiết thời gian thực tại TP. Hồ Chí Minh.\nXử lý dữ liệu JSON trả về từ API thành định dạng bảng (dạng DataFrame của pandas).\nChuyển dữ liệu sang chuỗi CSV.\nUpload file CSV này lên Amazon S3 (bucket vùng raw) để phục vụ bước ETL tiếp theo.\nLuồng xử lý gồm 2 task chính:\nextract_api_data: gọi API → chuẩn hóa JSON → build DataFrame → chuyển thành CSV string.\nupload_to_S3: lấy dữ liệu CSV từ XCom → upload lên bucket/raw/weather_api_data.csv.\nDAG này chạy mỗi giờ, đảm bảo dữ liệu luôn cập nhật.\n2. transform_redshift_load.py – DAG điều phối quá trình ETL DAG này tên là transform_redshift_dag, dùng để:\nChờ DAG openweather_api_dag chạy thành công, thông qua ExternalTaskSensor.\nSau đó gọi AWS Glue Job để thực hiện bước ETL: chuyển đổi dữ liệu và nạp vào Redshift.\nDAG sử dụng GlueJobOperator để gọi script ETL transform.py nằm ở ngoài môi trường Airflow (trong thư mục S3 dành riêng cho Glue)transform_redshift_load.\nCấu trúc gồm 2 task:\nwait_openweather_api: đợi DAG trước hoàn tất.\ntransform_task: gọi GlueJobOperator, trỏ đến script ETL trong S3 (transform.py).\n3. requirements.txt Nếu cần cài thêm thư viện cho Airflow (như numpy, pandas, requests), file này sẽ liệt kê các package cần cài. Được upload lên S3 tại thư mục cấu hình requirements/ để MWAA đọc khi khởi tạo môi trường và chạy đúng các task.\nnumpy==1.24.3\rpandas==1.3.5\rrequests==2.31.0\rVề môi trường thực thi của AWS Glue: transform.py – Script ETL chạy trong AWS Glue File này không nằm trong môi trường Airflow, mà được AWS Glue sử dụng để xử lý dữ liệu bằng PySpark.\nScript xử lý dữ liệu bằng PySpark (Spark trong Python).\nThực hiện các thao tác như: làm sạch dữ liệu, đổi tên trường, chuyển đổi kiểu dữ liệu, kiểm tra null, v.v.\nKết quả được nạp trực tiếp vào Amazon Redshift thông qua kết nối JDBC.\nĐiểm quan trọng: script này chạy với Spark bên trong Glue nhưng được viết bằng Python (PySpark), dễ tích hợp và chỉnh sửa.\nTóm lại Các file môi trường đóng vai trò là bộ não điều phối toàn bộ pipeline. Chúng gồm:\nCác DAG định nghĩa luồng công việc (gọi API, ETL, load).\nCác script xử lý dữ liệu bằng PySpark.\nCác file cấu hình phụ trợ như requirements.txt.\nViệc chuẩn bị chính xác các file này giúp MWAA hoạt động mượt mà, đảm bảo tính tự động, chính xác và có thể mở rộng cho hệ thống ETL thời tiết.\n"
},
{
	"uri": "//localhost:1313/vi/1-introduce/",
	"title": "Giới thiệu",
	"tags": [],
	"description": "",
	"content": "\rNguyễn Ngọc Anh Thư – 22133059\rTrường Đại học Sư phạm Kỹ thuật TP. Hồ Chí Minh\rXây dựng kiến trúc xử lý tự động và trực quan hóa dữ liệu thời gian thực trên AWS\rBáo cáo này trình bày quá trình xây dựng một hệ thống xử lý và trực quan hóa dữ liệu thời gian thực trên AWS, bao gồm các nội dung chính sau:\rGiới thiệu: Sơ lược về các nội dung liên quan tới đề tài.\rCác công cụ sử dụng \u0026 Sơ đồ kiến trúc: Trình bày các dịch vụ AWS, công cụ hỗ trợ và sơ đồ tổng thể của hệ thống.\rCác bước thực hiện quá trình ETL tự động: Hướng dẫn chi tiết từng bước từ chuẩn bị môi trường, triển khai hạ tầng, thiết lập kết nối đến thực thi ETL.\rPhân tích \u0026 trực quan hóa: Mô tả cách phân tích dữ liệu với AWS Redshift và xây dựng dashboard trực quan với AWS QuickSight.\rDọn dẹp tài nguyên: Hướng dẫn xóa các tài nguyên AWS sau khi hoàn thành để tối ưu chi phí.\rThời gian thực hiện\rLấy dữ liệu: 1 ngày\rThực hiện các thao tác: 2 - 3 tiếng (tùy vào độ quen thuộc với các công cụ)\rChú ý: Dữ liệu chỉ đủ để phân tích, nếu nhiều hơn có thể phát sinh thêm nhiều chi phí.\rĐiều kiện tiên quyết\rCó tài khoản AWS và quyền truy cập các dịch vụ cần thiết (S3, IAM, Glue, Redshift, MWAA, QuickSight,...).\rĐã cài đặt Python và các thư viện liên quan (requests, boto3,...).\rMáy tính kết nối internet ổn định.\rKiến thức cơ bản về AWS và dòng lệnh.\rKhoản phí ước tính:Trong khoảng từ 20$ tới 30$\r"
},
{
	"uri": "//localhost:1313/vi/4-analysis-visualization/4.1-redshift/",
	"title": "Phân tích với Amazon Redshift",
	"tags": [],
	"description": "",
	"content": "Tóm lại, với Amazon Redshift bạn có thể chạy các truy vấn phân tích dữ liệu thời gian thực, tận dụng tính năng phân phối, sắp xếp và vật liệu hóa để tối ưu chi phí và hiệu năng.\n"
},
{
	"uri": "//localhost:1313/vi/2-architecture/2.1-tools/",
	"title": "Các công cụ được sử dụng",
	"tags": [],
	"description": "",
	"content": "1. OpenWeather API OpenWeather API là dịch vụ cung cấp dữ liệu thời tiết thời gian thực thông qua các RESTful endpoint. Người dùng có thể truy xuất các thông tin như nhiệt độ, độ ẩm, áp suất, v.v. bằng cách gửi yêu cầu HTTP và nhận về dữ liệu ở định dạng JSON hoặc XML.\n2. Python Script Python là ngôn ngữ lập trình phổ biến, mạnh mẽ trong xử lý dữ liệu. Trong hệ thống này, Python script được sử dụng để:\nGọi API (ví dụ: OpenWeather API) để lấy dữ liệu.\nPhân tích cú pháp (parse) dữ liệu JSON trả về.\nLàm sạch, chuẩn hóa dữ liệu (chuyển đổi múi giờ, đổi tên trường, v.v.).\nXuất dữ liệu ra các định dạng như CSV, Parquet, hoặc JSON.\nĐẩy dữ liệu lên Amazon S3 để lưu trữ.\n3. Amazon S3 Amazon S3 là dịch vụ lưu trữ đối tượng (object storage) của AWS, cho phép lưu trữ và truy xuất dữ liệu với độ bền và khả năng mở rộng cao. Trong kiến trúc này, S3 được dùng để:\nLưu trữ dữ liệu thô (raw data) do Python script tạo ra.\nLưu trữ các file định nghĩa DAGs cho Airflow.\nLưu trữ các file requirements.txt để quản lý dependencies cho Airflow.\n4. AWS Glue AWS Glue là dịch vụ ETL (Extract, Transform, Load) serverless của AWS, hỗ trợ tự động hóa việc phát hiện schema, làm sạch, biến đổi và nạp dữ liệu. Các thành phần chính:\nGlue Crawler: Tự động quét dữ liệu trên S3, phát hiện schema và tạo metadata table trong Data Catalog.\nGlue ETL Job: Chạy mã Scala hoặc Python (Spark) để xử lý, biến đổi dữ liệu và xuất kết quả sang S3 hoặc Redshift.\n5. Amazon Redshift Amazon Redshift là dịch vụ data warehouse trên nền tảng đám mây, tối ưu cho các truy vấn phân tích dữ liệu lớn (OLAP). Redshift hỗ trợ lưu trữ dữ liệu theo mô hình star/snowflake schema, cho phép thực hiện các truy vấn phức tạp với hiệu năng cao, phù hợp cho các bài toán BI, phân tích dữ liệu.\n6. Apache Airflow (MWAA) Apache Airflow là nền tảng mã nguồn mở dùng để lập lịch, điều phối và giám sát các workflow (luồng công việc) phức tạp. Trên AWS, Airflow được cung cấp dưới dạng dịch vụ Managed Workflows for Apache Airflow (MWAA). Airflow sử dụng các DAGs (Directed Acyclic Graphs) để định nghĩa các bước xử lý dữ liệu, hỗ trợ tự động hóa, retry, alert, và tích hợp với nhiều dịch vụ AWS.\n7. Amazon QuickSight Amazon QuickSight là dịch vụ BI (Business Intelligence) trên AWS, hỗ trợ kết nối đến nhiều nguồn dữ liệu như Redshift, S3,\u0026hellip; QuickSight cho phép xây dựng dashboard, biểu đồ, báo cáo tương tác với giao diện kéo-thả, hỗ trợ phân tích và trực quan hóa dữ liệu cho người dùng doanh nghiệp.\n8. AWS CloudFormation, IAM, VPC, Secrets Manager CloudFormation: Dịch vụ quản lý hạ tầng dưới dạng mã (Infrastructure as Code), giúp tự động hóa việc triển khai và quản lý tài nguyên AWS.\nIAM (Identity and Access Management): Quản lý quyền truy cập, xác thực và phân quyền cho người dùng/dịch vụ.\nVPC (Virtual Private Cloud): Tạo mạng ảo riêng biệt trên AWS, kiểm soát truy cập và bảo mật cho các tài nguyên.\nSecrets Manager: Lưu trữ, quản lý và truy xuất thông tin nhạy cảm (API key, mật khẩu, v.v.) một cách an toàn.\n"
},
{
	"uri": "//localhost:1313/vi/2-architecture/",
	"title": "Công cụ sử dụng và Kiến trúc",
	"tags": [],
	"description": "",
	"content": "Hệ thống ETL thời tiết được xây dựng trên nền tảng AWS, kết hợp với các công cụ mã nguồn mở như Python và Apache Airflow. Dữ liệu thời tiết được lấy từ OpenWeather API, xử lý bởi script Python và điều phối bằng Airflow (MWAA). Dữ liệu thô được lưu tạm vào Amazon S3, sau đó được AWS Glue xử lý và chuẩn hóa trước khi nạp vào kho dữ liệu Amazon Redshift. Cuối cùng, Amazon QuickSight được dùng để trực quan hóa dữ liệu phục vụ phân tích. Toàn bộ hạ tầng được triển khai tự động qua CloudFormation và bảo mật bởi IAM, VPC, và Secrets Manager. Sơ đồ kiến trúc thể hiện rõ luồng dữ liệu và vai trò của từng thành phần trong hệ thống.\nNội dung chính: 2.1 Các công cụ sử dụng\n2.2 Sơ đồ kiến trúc\n"
},
{
	"uri": "//localhost:1313/vi/3-etl-process/3.2-infra-as-code/",
	"title": "Triển khai hạ tầng bằng mã nguồn",
	"tags": [],
	"description": "",
	"content": "Phần này mô tả chi tiết cách sử dụng AWS CloudFormation để tự động khởi tạo và quản lý toàn bộ hạ tầng cần thiết cho pipeline ETL.\n1. Giới thiệu template File: template-workshop-anhthu.yaml\nMục đích: Định nghĩa toàn bộ tài nguyên AWS dưới dạng code, bao gồm:\nS3 Buckets:\nBuckets để lưu DAGs \u0026amp; requirements cho MWAA.\nBuckets để lưu dữ liệu thô (raw data).\nIAM Roles \u0026amp; Policies:\nRole cho MWAA (đọc DAG, ghi S3, submit Glue).\nRole cho Glue (đọc S3, ghi Redshift).\nVPC \u0026amp; Networking:\nVPC, Subnets, Security Groups cho MWAA, Glue và Redshift. MWAA Environment: tham chiếu tới S3, IAM Role, network.\nAWS Glue Resources: Connection đến Redshift, (tùy chọn) Crawler, Job definition.\nAmazon Redshift Cluster: cấu hình node type, node count, database, parameter group.\nSecrets Manager: lưu trữ OpenWeather API key và credentials Redshift.\nOutputs: Endpoint của Redshift, tên buckets, ARN của Roles.\nMẫu template này áp dụng chuẩn YAML cho CloudFormation, giúp versioning, review và reuse dễ dàng.\n2. Những chỗ cần chú ý khi tái sử dụng code Trước khi deploy, bạn cần kiểm tra và thay thế các giá trị sau trong phần Parameters hoặc trực tiếp trong template:\nCác tên của các thành phần dịch vụ và các địa chỉ IP, API keys, ARN url, đường dẫn tới các dịch vụ sử dụng, các loại phù hợp. Chọn đúng lớp node (node class) và số lượng node, vùng có giá phù hợp khối lượng dữ liệu.\n3. Các bước triển khai (Deployment) Bước 1: Tạo CloudFormation Stack Đăng nhập AWS Console → chọn CloudFormation. Chọn Create stack → With new resources (standard). Chọn Upload a template file, tải lên template-workshop-anhthu.yaml. Nhấn Next. Nhấn Next qua phần Options (tuỳ chọn thêm Tags). Chọn “I acknowledge that AWS CloudFormation might create IAM resources”. Nhấn Create stack và theo dõi tab Events đến khi trạng thái là CREATE_COMPLETE. Bước 2: Kiểm tra tài nguyên đã tạo S3 IAM Kiểm tra hai IAM Role (MWAA, Glue) với đúng policy. VPC \u0026amp; Security Groups Trong VPC console, xác nhận Subnets và Security Groups cho phép: MWAA truy xuất S3 Glue truy xuất S3 \u0026amp; Redshift Redshift cho phép kết nối port 5439 MWAA Environment Console MWAA hiển thị environment mới, status Available. AWS Glue Xác nhận Connection redshift-demo-connection. Redshift Console Redshift hiển thị cluster available. Tạo bao nhiêu check bấy nhiêu để lúc làm không bị thiếu môi trường hay kết nối (nếu có phải xóa đi tạo stack lại) và có thể note lại để lúc xóa đảm bảo không xóa thiếu dịch vụ tránh tăng các khoản phí không đáng nói.\nBước 3: Kết nối và kiểm thử Redshift "
},
{
	"uri": "//localhost:1313/vi/2-architecture/2.2-architecture/",
	"title": "Xây dựng Sơ đồ Kiến trúc",
	"tags": [],
	"description": "",
	"content": "Sơ đồ Kiến trúc Kiến trúc này mô tả một quy trình thu thập, xử lý, lưu trữ, phân tích và trực quan hóa dữ liệu thời tiết thông qua các dịch vụ AWS và công cụ mã nguồn mở. Ban đầu, người dùng gửi yêu cầu thông tin thời tiết. Một Python script được đóng gói và lưu trữ vào môi trường Amazon S3, để Apache Airflow (MWAA) tự động gọi thực thi theo lịch định sẵn. Python script này sử dụng thư viện PySpark (Spark được tích hợp sẵn trong script Python) để gọi API của OpenWeather, lấy dữ liệu thời tiết theo thời gian thực, xử lý sơ bộ và lưu vào bucket Amazon S3 dưới dạng dữ liệu thô. Tiếp theo, AWS Glue Job tiến hành các bước ETL nâng cao, làm sạch, chuẩn hóa dữ liệu và nạp vào Amazon Redshift, sẵn sàng cho việc phân tích chuyên sâu. Sau đó, Amazon QuickSight kết nối với Redshift để tạo ra dashboard tương tác, trực quan hóa dữ liệu giúp người dùng dễ dàng phân tích và ra quyết định.\nToàn bộ hạ tầng và các dịch vụ này được quản lý và triển khai tự động bằng AWS CloudFormation, đồng thời được bảo mật chặt chẽ bởi AWS IAM, VPC và Secrets Manager, giúp kiểm soát truy cập và lưu trữ an toàn các thông tin nhạy cảm trong hệ thống.\nSơ đồ dưới đây minh họa tổng thể kiến trúc hệ thống trên AWS: "
},
{
	"uri": "//localhost:1313/vi/3-etl-process/",
	"title": "Quá trình ETL tự động",
	"tags": [],
	"description": "",
	"content": "Trong kiến trúc thu thập và xử lý dữ liệu thời tiết sử dụng AWS, phần quan trọng nhất nằm ở quy trình ETL (Extract – Transform – Load) tự động. Đây là nơi toàn bộ dữ liệu được lấy về từ OpenWeather API, xử lý, lưu trữ và chuyển hóa thành thông tin có thể phân tích được.\nLuồng xử lý tổng thể gồm 4 bước chính: 1. Chuẩn bị môi trường Viết script Python để gọi API và xử lý dữ liệu ban đầu.\nĐóng gói các file này vào S3 để Airflow (MWAA) có thể chạy tự động.\n2. Triển khai hạ tầng tự động Sử dụng AWS CloudFormation để triển khai toàn bộ tài nguyên cần thiết như S3, MWAA, IAM, Glue, Redshift\u0026hellip;\n3. Cấu hình Airflow Thiết lập biến (Variables) và kết nối (Connections) trong Airflow.\nUpload DAG và các tệp cần thiết lên đúng vị trí trong S3.\n4. Thực thi ETL Apache Airflow tự động kích hoạt pipeline theo lịch trình.\nGọi API → lưu vào S3 → xử lý bằng Glue → lưu vào Redshift → trực quan hóa bằng QuickSight.\nTại sao cần ETL tự động? Tiết kiệm thời gian: Không cần thủ công tải dữ liệu hằng ngày.\nChuẩn hóa dữ liệu: Đảm bảo dữ liệu có định dạng sạch, đồng nhất, có thể phân tích.\nTích hợp dễ dàng: Với các công cụ phân tích như Amazon Redshift \u0026amp; QuickSight.\nCó thể mở rộng: Dễ dàng áp dụng cho nhiều loại dữ liệu khác nhau, không chỉ thời tiết.\nNội dung chính: 3.1 Về các file môi trường\n3.2 Triển khai hạ tầng bằng mã nguồn\n3.3 Thiết lập biến, tạo kết nối và upload môi trường\n3.4 Quá trình thực hiện ETL\n"
},
{
	"uri": "//localhost:1313/vi/3-etl-process/3.3-setup-variables-connection/",
	"title": "Thiết lập biến, tạo kết nối và upload môi trường",
	"tags": [],
	"description": "",
	"content": "3.3 Thiết lập biến, tạo kết nối và upload môi trường Bước 1: Tạo Variables trong Airflow Mở Airflow UI → Admin → Variables → Create. Tạo biến: Key: key\nValue: \u0026lt;YOUR_OPENWEATHER_API_KEY\u0026gt; Bước 2: Tạo Connections trong Airflow Mở Airflow UI → Admin → Connections → Create. Tạo AWS_CONN (kết nối tới AWS): Conn Id: AWS_CONN Conn Type: Amazon Web Services Login: \u0026lt;AWS_ACCESS_KEY_ID\u0026gt; Password: \u0026lt;AWS_SECRET_ACCESS_KEY\u0026gt; Chú ý cách lấy access key\nLưu ý: Trong DAG transform_redshift_load.py, tham số\naws_conn_id=\u0026#39;AWS_CONN\u0026#39; create_job_kwargs={\u0026#39;Connections\u0026#39;:{\u0026#39;Connections\u0026#39;:[\u0026#39;redshift-demo-connection\u0026#39;]}} Bước 3: Tạo 2 S3 Buckets để chứa dữ liệu raw khi thu thập được và các nội dung trong Glue thực hiện Bước 4: Kết nối db để thực hiện đẩy dữ liệu vào trong Redshift "
},
{
	"uri": "//localhost:1313/vi/4-analysis-visualization/",
	"title": "Phân tích và trực quan hóa",
	"tags": [],
	"description": "",
	"content": "1. Giới thiệu về dữ liệu thu được Sau khi pipeline ETL chạy xong, bạn sẽ có bảng public.weather_data trong Redshift chứa các cột chính sau:\nCột Kiểu dữ liệu Mô tả dt timestamp Thời điểm ghi nhận (chuyển từ epoch) dt_iso timestamp Thời điểm ghi nhận ở định dạng ISO temp double Nhiệt độ trung bình (°C) feels_like double Nhiệt độ cảm nhận (°C) pressure integer Áp suất (hPa) humidity integer Độ ẩm (%) wind_speed double Tốc độ gió (m/s) weather_main varchar Tình trạng thời tiết chính (Clear, Clouds, Rain,…) weather_description varchar Mô tả chi tiết hơn (few clouds, light rain,…) city varchar Tên thành phố country varchar Quốc gia ingest_time timestamp Thời điểm dữ liệu được ETL vào kho (UTC) Dữ liệu được thu thập mỗi giờ, lưu trữ lịch sử liên tục, giúp phân tích xu hướng và biến động thời tiết theo thời gian.\n4.2 Phân tích trên Amazon Redshift Tính toán chỉ số trung bình \u0026amp; biến động Ví dụ: trung bình nhiệt độ hàng ngày, biến động áp suất theo giờ. Phân loại \u0026amp; lọc theo điều kiện Ví dụ: đếm số lần xảy ra “Rain” trong tuần, mức độ ẩm vượt ngưỡng. Xác định xu hướng \u0026amp; seasonality Ví dụ: tăng/giảm nhiệt độ theo mùa, chu kỳ mưa. Kết hợp với dữ liệu kinh doanh Ví dụ: so sánh lưu lượng khách tham quan ngoài trời với điều kiện thời tiết. Redshift với khả năng xử lý OLAP cho phép bạn chạy các truy vấn tổng hợp, phân tích thời gian lớn một cách nhanh chóng và hiệu quả.\n4.3 Trực quan hóa với Amazon QuickSight Dashboard tương tác Dễ dàng kéo thả để tạo biểu đồ line chart (nhiệt độ, độ ẩm), bar chart (số ngày mưa), gauge chart (áp suất trung bình). Filter \u0026amp; drill-down Cho phép chọn khoảng thời gian, thành phố, điều kiện thời tiết để phân tích chi tiết. Alerts \u0026amp; insights tự động Sử dụng tính năng “Anomaly Detection” và “Notifications” để cảnh báo khi nhiệt độ/độ ẩm bất thường. Chia sẻ \u0026amp; nhúng Xuất report, chia sẻ link với đồng nghiệp hoặc nhúng dashboard vào ứng dụng nội bộ. Kết hợp Redshift và QuickSight, bạn có thể triển khai một giải pháp “end-to-end” từ thu thập dữ liệu, phân tích chuyên sâu đến trực quan hóa linh hoạt, hỗ trợ ra quyết định nhanh chóng dựa trên dữ liệu thời tiết.\nNội dung chính: 4.1 Phân tích với AWS Redshift\n4.2 Trực quan hóa với AWS Quicksight\n"
},
{
	"uri": "//localhost:1313/vi/3-etl-process/3.4-etl-execution/",
	"title": "Quá trình thực hiện ETL",
	"tags": [],
	"description": "",
	"content": "Toàn bộ pipeline ETL đã được cấu hình để chạy tự động hàng giờ. Dưới đây là các bước chính khi bạn “go-live”:\nBước 1: Bật (unpause) các DAG trong Airflow Mở Airflow UI → DAGs. Tìm hai DAG openweather_api_dag và transform_redshift_dag. Chuyển toggle từ Off → On để kích hoạt tự động chạy theo schedule_interval (mỗi giờ). Xác nhận cột Schedule hiển thị “@hourly” và status “healthy”. Lưu ý: Bạn cũng có thể bật DAGs qua CLI:\nairflow dags unpause openweather_api_dag airflow dags unpause transform_redshift_dag Bước 2: Theo dõi, đợi và kiểm tra (nếu lỗi) Theo dõi lần chạy đầu\nVào tab Graph hoặc Tree view của openweather_api_dag. Quan sát Task extract_api_data → upload_to_S3. Chờ trạng thái từng task chuyển thành success (màu xanh lá). Kiểm tra dữ liệu thô trên S3\nVào S3 Console → bucket raw-data. Xác nhận file raw/weather_api_data.csv đã được viết mới (timestamp gần nhất). Chờ DAG ETL tiếp theo\nTương tự với transform_redshift_dag, quan sát task wait_openweather_api → transform_task. Khi thành công, Glue Job sẽ chạy và nạp dữ liệu vào Redshift. Kiểm tra dữ liệu trong Redshift\nVào Query Editor → chạy: SELECT COUNT(*) FROM public.weather_data; Xác nhận kết quả (≥ 0) và dữ liệu mới nhất có ingest_time phù hợp. Xử lý lỗi (nếu có)\nNếu task bất kỳ fail, click vào task → Logs để xem chi tiết. Thông thường lỗi có thể do: API key không hợp lệ (Task extract_api_data). Bucket S3 sai đường dẫn hoặc quyền truy cập. Glue Job script lỗi khi parse hoặc connect Redshift. IAM Role thiếu policy. Sửa code hoặc cấu hình, commit \u0026amp; sync lại DAGs/scripts, then clear task → rerun. Riêng mình gặp phải lỗi Có thể thêm Giám sát dài hạn nếu lấy dữ liệu lâu. Ex: Dùng CloudWatch \u0026amp; Cost Explorer để theo dõi logs và chi phí.\nTóm lại, chỉ cần bật DAGs, sau đó Airflow tự động chạy, theo dõi kết quả và can thiệp khi có lỗi để đảm bảo pipeline vận hành ổn định và dữ liệu luôn được cập nhật.\n"
},
{
	"uri": "//localhost:1313/vi/4-analysis-visualization/4.2-quicksight/",
	"title": "",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "//localhost:1313/vi/5-cleanup/",
	"title": "",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "//localhost:1313/vi/6-in-conclusion/",
	"title": "",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "//localhost:1313/vi/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "//localhost:1313/vi/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]