[
{
	"uri": "//localhost:1313/vi/",
	"title": "Xây dựng kiến trúc xử lý tự động và trực quan hóa dữ liệu thời gian thực trên AWS",
	"tags": [],
	"description": "",
	"content": "Xây dựng kiến trúc xử lý tự động và trực quan hóa dữ liệu thời gian thực trên AWS Họ tên: Nguyễn Ngọc Anh Thư\nMSSV: 22133059\nEmail: ngocanhthugialai@gmail.com\nTrường: Đại học Sư phạm Kỹ thuật TP.HCM (HCMUTE)\nTổng quan Đề tài \u0026ldquo;Xây dựng kiến trúc xử lý tự động và trực quan hóa dữ liệu thời gian thực trên AWS\u0026rdquo; là sự tâm huyết và nỗ lực nhằm xây dựng một hệ thống toàn diện, tự động hóa việc thu thập, xử lý và hiển thị trực quan các luồng dữ liệu thời gian thực dựa trên nền tảng Amazon Web Services (AWS). Với mục tiêu đáp ứng nhu cầu phân tích và ra quyết định nhanh chóng, hiệu quả dựa trên dữ liệu mới nhất, hệ thống được thiết kế bao gồm các thành phần chính: thu thập dữ liệu thời tiết từ API của OpenWeather bằng Python và lưu trữ vào AWS S3; orchestration thông qua Amazon Managed Workflows for Apache Airflow (MWAA); xử lý và chuyển đổi dữ liệu bằng AWS Glue; lưu trữ dữ liệu vào kho dữ liệu AWS Redshift; và cuối cùng trực quan hóa kết quả bằng AWS QuickSight trên dashboard trực quan.\nViệc triển khai hạ tầng được thực hiện thông qua mã nguồn (Infrastructure as Code - IaC) sử dụng AWS CloudFormation, đảm bảo tính nhất quán, tự động hóa và dễ dàng quản lý, mở rộng khi có nhu cầu thay đổi hoặc bổ sung tài nguyên. Các chính sách bảo mật và quản lý truy cập cũng được thiết lập thông qua AWS IAM và môi trường bảo mật mạng riêng ảo (VPC) nhằm đảm bảo an toàn cho hệ thống.\nEm rất hy vọng báo cáo này không chỉ phản ánh chân thực quá trình thực hiện của em, mà còn là một tài liệu thân thiện, dễ hiểu, đồng hành cùng các bạn sinh viên mới tiếp cận AWS hoặc các mô hình phát triển hiện đại. Mong rằng, qua đây em có thể truyền tải được sự nhiệt huyết, niềm đam mê với công nghệ và hỗ trợ các bạn tự tin hơn trên hành trình khám phá và ứng dụng AWS.\nThời gian thực hiện Lấy dữ liệu: 1 ngày Dữ liệu chỉ đủ để phân tích, nếu nhiều hơn có thể phát sinh thêm nhiều chi phí\nThực hiện các thao tác: 2 - 3 tiếng (tùy vào độ quen thuộc với các công cụ) Nội dung Giới thiệu Các công cụ sử dụng và Sơ đồ kiến trúc Các công cụ sử dụng Sơ đồ kiến trúc Các bước thực hiện quá trình ETL tự động Về các file môi trường Triển khai hạ tầng bằng mã nguồn Thiết lập biến, tạo kết nối và upload môi trường Quá trình thực hiện ETL Phân tích và trực quan hóa Phân tích với AWS Redshift Trực quan hóa với AWS Quicksight Dọn dẹp tài nguyên Kết luận "
},
{
	"uri": "//localhost:1313/vi/3-etl-process/3.1-env-files/",
	"title": "Các file môi trường",
	"tags": [],
	"description": "",
	"content": "Trong hệ thống ETL tự động trên AWS, các file môi trường là tập hợp mã nguồn Python và cấu hình được sử dụng bởi Apache Airflow (MWAA) để điều phối toàn bộ quy trình thu thập – xử lý – lưu trữ dữ liệu. Các file này được lưu trữ trong Amazon S3, và được phân chia rõ ràng theo vai trò: môi trường của Airflow và môi trường thực thi của AWS Glue.\nVề môi trường của Airflow 1. openweather_api.py – DAG thu thập dữ liệu thời tiết Đây là file đầu tiên cần được triển khai trong môi trường Airflow. File này định nghĩa một DAG tên là openweather_api_dag, có nhiệm vụ:\nGọi OpenWeather API để lấy dữ liệu thời tiết thời gian thực tại TP. Hồ Chí Minh. api_endpoint = \u0026#39;https://api.openweathermap.org/data/2.5/weather\u0026#39;\rapi_params = {\r\u0026#39;q\u0026#39;: \u0026#39;Thanh pho Ho Chi Minh, VN\u0026#39;, # Địa điểm lấy dữ liệu\r\u0026#39;appid\u0026#39;: Variable.get(\u0026#39;key\u0026#39;), # API key lấy từ Airflow Variable (bảo mật)\r\u0026#39;units\u0026#39;: \u0026#39;metric\u0026#39; # Đơn vị nhiệt độ\r} Xử lý dữ liệu JSON trả về từ API thành định dạng bảng (dạng DataFrame của pandas).\nChuyển dữ liệu sang chuỗi CSV.\nUpload file CSV này lên Amazon S3 (bucket vùng raw) để phục vụ bước ETL tiếp theo.\nLuồng xử lý gồm 2 task chính: extract_api_data \u0026raquo; upload_to_s3\nextract_api_data: gọi API → chuẩn hóa JSON → build DataFrame → chuyển thành CSV string. #Lấy dữ liệu từ API, đẩy lên XCom\rextract_api_data = PythonOperator(\rtask_id=\u0026#39;extract_api_data\u0026#39;,\rpython_callable=extract_openweather_data, #Hàm được định nghĩa trong file code\rprovide_context=True\r) upload_to_S3: lấy dữ liệu CSV từ XCom → upload lên \u0026hellip;/raw/weather_api_data.csv. #Upload dữ liệu lên S3 (bucket raw)\rupload_to_s3 = S3CreateObjectOperator(\rtask_id=\u0026#39;upload_to_S3\u0026#39;,\raws_conn_id=\u0026#39;AWS_CONN\u0026#39;,\rs3_bucket=\u0026#39;airflowoutputtos3bucket-324413232937-us-east-1\u0026#39;,\rs3_key=\u0026#39;raw/weather_api_data.csv\u0026#39;,\rdata=\u0026#34;{{ ti.xcom_pull(key=\u0026#39;final_data\u0026#39;) }}\u0026#34;,\rreplace=True\r) DAG này chạy mỗi giờ, đảm bảo dữ liệu luôn cập nhật.\nschedule_interval=\u0026#39;@hourly\u0026#39; 2. transform_redshift_load.py – DAG điều phối quá trình ETL DAG này tên là transform_redshift_dag, dùng để:\nChờ DAG openweather_api_dag chạy thành công, thông qua ExternalTaskSensor.\nSau đó gọi AWS Glue Job để thực hiện bước ETL: chuyển đổi dữ liệu và nạp vào Redshift.\nDAG sử dụng GlueJobOperator để gọi script ETL transform.py nằm ở ngoài môi trường Airflow (trong thư mục S3 dành riêng cho Glue)transform_redshift_load.\nCấu trúc gồm 2 task: wait_openweather_api \u0026raquo; transform_task\nwait_openweather_api: đợi DAG trước hoàn tất. #Chờ DAG openweather_api_dag (lấy dữ liệu từ API) hoàn thành thành công trước khi ETL\rwait_openweather_api = ExternalTaskSensor(\rtask_id=\u0026#39;wait_openweather_api\u0026#39;,\rexternal_dag_id=\u0026#39;openweather_api_dag\u0026#39;, # Tên DAG lấy dữ liệu từ OpenWeather API\rexternal_task_id=None, # Chờ toàn bộ DAG thành công\rtimeout=2000, # Timeout (giây)\rdag=dag,\rmode=\u0026#39;reschedule\u0026#39;,\rallowed_states=[\u0026#34;success\u0026#34;]\r) transform_task: gọi GlueJobOperator, trỏ đến script ETL trong S3 (transform.py). #Trigger Glue Job để ETL dữ liệu từ S3 vào Redshift\rtransform_task = GlueJobOperator(\rtask_id=\u0026#39;transform_task\u0026#39;,\rjob_name=\u0026#39;glue_transform_task\u0026#39;, # Tên Glue Job đã tạo sẵn\rscript_location=\u0026#39;s3://aws-glue-assets-324413232937-us-east-1/scripts/transform.py\u0026#39;, # Đường dẫn script ETL\raws_conn_id=\u0026#39;AWS_CONN\u0026#39;, # Kết nối AWS trong Airflow\rregion_name=\u0026#34;us-east-1\u0026#34;,\riam_role_name=\u0026#39;data-workshop-anhthu-RedshiftIamRole-wTwBvtcZavdl\u0026#39;, # IAM Role cho Glue\rcreate_job_kwargs={\r\u0026#34;GlueVersion\u0026#34;: \u0026#34;4.0\u0026#34;,\r\u0026#34;NumberOfWorkers\u0026#34;: 2,\r\u0026#34;WorkerType\u0026#34;: \u0026#34;G.1X\u0026#34;,\r\u0026#34;Connections\u0026#34;: {\u0026#34;Connections\u0026#34;: [\u0026#34;redshift-demo-connection\u0026#34;]} # Kết nối Glue với Redshift\r},\rdag=dag,\r) \u0026lsquo;start_date\u0026rsquo; và \u0026lsquo;schedule_interval\u0026rsquo; phải giống nhau, nói chung là định nghĩa DAG Airflow và các thông số mặc định cho các task trong DAG phải đồng nhất với nhau mới đảm bảo được Airflow sẽ chạy thành công như ý muốn.\n3. requirements.txt Nếu cần cài thêm thư viện cho Airflow (như numpy, pandas, requests), file này sẽ liệt kê các package cần cài. Được upload lên S3 tại thư mục cấu hình requirements/ để MWAA đọc khi khởi tạo môi trường và chạy đúng các task.\nnumpy==1.24.3\rpandas==1.3.5\rrequests==2.31.0 Về môi trường thực thi của AWS Glue: transform.py – Script ETL chạy trong AWS Glue File này không nằm trong môi trường Airflow, mà được AWS Glue sử dụng để xử lý dữ liệu bằng PySpark.\nScript xử lý dữ liệu bằng PySpark (Spark trong Python).\nThực hiện các thao tác như: làm sạch dữ liệu, đổi tên trường, chuyển đổi kiểu dữ liệu, kiểm tra null, v.v.\nKết quả được nạp trực tiếp vào Amazon Redshift thông qua kết nối JDBC.\nĐiểm quan trọng: script này chạy với Spark bên trong Glue nhưng được viết bằng Python (PySpark), dễ tích hợp và chỉnh sửa.\nTóm lại Các file môi trường đóng vai trò là bộ não điều phối toàn bộ pipeline. Chúng gồm:\nCác DAG định nghĩa luồng công việc (gọi API, ETL, load).\nCác script xử lý dữ liệu bằng PySpark.\nCác file cấu hình phụ trợ như requirements.txt.\nViệc chuẩn bị chính xác các file này giúp MWAA hoạt động mượt mà, đảm bảo tính tự động, chính xác và có thể mở rộng cho hệ thống ETL thời tiết.\n"
},
{
	"uri": "//localhost:1313/vi/1-introduce/",
	"title": "Giới thiệu",
	"tags": [],
	"description": "",
	"content": "\rNguyễn Ngọc Anh Thư – 22133059\rTrường Đại học Sư phạm Kỹ thuật TP. Hồ Chí Minh\rXây dựng kiến trúc xử lý tự động và trực quan hóa dữ liệu thời gian thực trên AWS\rBáo cáo này trình bày quá trình xây dựng một hệ thống xử lý và trực quan hóa dữ liệu thời gian thực trên AWS, bao gồm các nội dung chính sau:\rGiới thiệu: Sơ lược về các nội dung liên quan tới đề tài.\rCác công cụ sử dụng \u0026 Sơ đồ kiến trúc: Trình bày các dịch vụ AWS, công cụ hỗ trợ và sơ đồ tổng thể của hệ thống.\rCác bước thực hiện quá trình ETL tự động: Hướng dẫn chi tiết từng bước từ chuẩn bị môi trường, triển khai hạ tầng, thiết lập kết nối đến thực thi ETL.\rPhân tích \u0026 trực quan hóa: Mô tả cách phân tích dữ liệu với AWS Redshift và xây dựng dashboard trực quan với AWS QuickSight.\rDọn dẹp tài nguyên: Hướng dẫn xóa các tài nguyên AWS sau khi hoàn thành để tối ưu chi phí.\rThời gian thực hiện\rLấy dữ liệu: 1 ngày\rThực hiện các thao tác: 2 - 3 tiếng (tùy vào độ quen thuộc với các công cụ)\rChú ý: Dữ liệu chỉ đủ để phân tích, nếu nhiều hơn có thể phát sinh thêm nhiều chi phí.\rĐiều kiện tiên quyết\rCó tài khoản AWS và quyền truy cập các dịch vụ cần thiết (S3, IAM, Glue, Redshift, MWAA, QuickSight,...).\rĐã cài đặt Python và các thư viện liên quan (requests, boto3,...).\rMáy tính kết nối internet ổn định.\rKiến thức cơ bản về AWS và dòng lệnh.\rKhoản phí ước tính: Trong khoảng từ 20$ tới 30$\r"
},
{
	"uri": "//localhost:1313/vi/4-analysis-visualization/4.1-redshift/",
	"title": "Phân tích với Amazon Redshift",
	"tags": [],
	"description": "",
	"content": " Amazon Redshift là kho dữ liệu (data warehouse) OLAP, tối ưu cho truy vấn phân tích trên dữ liệu lớn.\nSử dụng mô hình phân phối (distribution key) và sắp xếp (sort key) để tối ưu I/O và hiệu năng.\nHỗ trợ SQL tiêu chuẩn, Materialized Views, và các công cụ ETL/BI tích hợp sẵn.\nVí dụ truy vấn với bảng public.weather_data Nhiệt độ trung bình hàng ngày\nSELECT DATE(dt) AS date, ROUND(AVG(temp), 2) AS avg_temp FROM public.weather_data GROUP BY date ORDER BY date; Tóm lại, với Amazon Redshift bạn có thể chạy các truy vấn phân tích dữ liệu thời gian thực, tận dụng tính năng phân phối, sắp xếp và vật liệu hóa để tối ưu chi phí và hiệu năng.\n"
},
{
	"uri": "//localhost:1313/vi/2-architecture/2.1-tools/",
	"title": "Các công cụ được sử dụng",
	"tags": [],
	"description": "",
	"content": "1. OpenWeather API OpenWeather API là dịch vụ cung cấp dữ liệu thời tiết thời gian thực thông qua các RESTful endpoint. Người dùng có thể truy xuất các thông tin như nhiệt độ, độ ẩm, áp suất, v.v. bằng cách gửi yêu cầu HTTP và nhận về dữ liệu ở định dạng JSON hoặc XML.\n2. Python Script Python là ngôn ngữ lập trình phổ biến, mạnh mẽ trong xử lý dữ liệu. Trong hệ thống này, Python script được sử dụng để:\nGọi API (ví dụ: OpenWeather API) để lấy dữ liệu.\nPhân tích cú pháp (parse) dữ liệu JSON trả về.\nLàm sạch, chuẩn hóa dữ liệu (chuyển đổi múi giờ, đổi tên trường, v.v.).\nXuất dữ liệu ra các định dạng như CSV, Parquet, hoặc JSON.\nĐẩy dữ liệu lên Amazon S3 để lưu trữ.\n3. Amazon S3 Amazon S3 là dịch vụ lưu trữ đối tượng (object storage) của AWS, cho phép lưu trữ và truy xuất dữ liệu với độ bền và khả năng mở rộng cao. Trong kiến trúc này, S3 được dùng để:\nLưu trữ dữ liệu thô (raw data) do Python script tạo ra.\nLưu trữ các file định nghĩa DAGs cho Airflow.\nLưu trữ các file requirements.txt để quản lý dependencies cho Airflow.\n4. AWS Glue AWS Glue là dịch vụ ETL (Extract, Transform, Load) serverless của AWS, hỗ trợ tự động hóa việc phát hiện schema, làm sạch, biến đổi và nạp dữ liệu. Các thành phần chính:\nGlue Crawler: Tự động quét dữ liệu trên S3, phát hiện schema và tạo metadata table trong Data Catalog.\nGlue ETL Job: Chạy mã Scala hoặc Python (Spark) để xử lý, biến đổi dữ liệu và xuất kết quả sang S3 hoặc Redshift.\n5. Amazon Redshift Amazon Redshift là dịch vụ data warehouse trên nền tảng đám mây, tối ưu cho các truy vấn phân tích dữ liệu lớn (OLAP). Redshift hỗ trợ lưu trữ dữ liệu theo mô hình star/snowflake schema, cho phép thực hiện các truy vấn phức tạp với hiệu năng cao, phù hợp cho các bài toán BI, phân tích dữ liệu.\n6. Apache Airflow (MWAA) Apache Airflow là nền tảng mã nguồn mở dùng để lập lịch, điều phối và giám sát các workflow (luồng công việc) phức tạp. Trên AWS, Airflow được cung cấp dưới dạng dịch vụ Managed Workflows for Apache Airflow (MWAA). Airflow sử dụng các DAGs (Directed Acyclic Graphs) để định nghĩa các bước xử lý dữ liệu, hỗ trợ tự động hóa, retry, alert, và tích hợp với nhiều dịch vụ AWS.\n7. Amazon QuickSight Amazon QuickSight là dịch vụ BI (Business Intelligence) trên AWS, hỗ trợ kết nối đến nhiều nguồn dữ liệu như Redshift, S3,\u0026hellip; QuickSight cho phép xây dựng dashboard, biểu đồ, báo cáo tương tác với giao diện kéo-thả, hỗ trợ phân tích và trực quan hóa dữ liệu cho người dùng doanh nghiệp.\n8. AWS CloudFormation, IAM, VPC, Secrets Manager CloudFormation: Dịch vụ quản lý hạ tầng dưới dạng mã (Infrastructure as Code), giúp tự động hóa việc triển khai và quản lý tài nguyên AWS.\nIAM (Identity and Access Management): Quản lý quyền truy cập, xác thực và phân quyền cho người dùng/dịch vụ.\nVPC (Virtual Private Cloud): Tạo mạng ảo riêng biệt trên AWS, kiểm soát truy cập và bảo mật cho các tài nguyên.\nSecrets Manager: Lưu trữ, quản lý và truy xuất thông tin nhạy cảm (API key, mật khẩu, v.v.) một cách an toàn.\n"
},
{
	"uri": "//localhost:1313/vi/2-architecture/",
	"title": "Công cụ sử dụng và Kiến trúc",
	"tags": [],
	"description": "",
	"content": "Hệ thống ETL thời tiết được xây dựng trên nền tảng AWS, kết hợp với các công cụ mã nguồn mở như Python và Apache Airflow. Dữ liệu thời tiết được lấy từ OpenWeather API, xử lý bởi script Python và điều phối bằng Airflow (MWAA). Dữ liệu thô được lưu tạm vào Amazon S3, sau đó được AWS Glue xử lý và chuẩn hóa trước khi nạp vào kho dữ liệu Amazon Redshift. Cuối cùng, Amazon QuickSight được dùng để trực quan hóa dữ liệu phục vụ phân tích. Toàn bộ hạ tầng được triển khai tự động qua CloudFormation và bảo mật bởi IAM, VPC, và Secrets Manager. Sơ đồ kiến trúc thể hiện rõ luồng dữ liệu và vai trò của từng thành phần trong hệ thống.\nĐược thực hiện trong vùng United States(N.Virginia) - us-east-1\nNội dung chính: 2.1 Các công cụ sử dụng\n2.2 Sơ đồ kiến trúc\n"
},
{
	"uri": "//localhost:1313/vi/3-etl-process/3.2-infra-as-code/",
	"title": "Triển khai hạ tầng bằng mã nguồn",
	"tags": [],
	"description": "",
	"content": "Phần này mô tả chi tiết cách sử dụng AWS CloudFormation để tự động khởi tạo và quản lý toàn bộ hạ tầng cần thiết cho pipeline ETL.\n1. Giới thiệu template File: template-workshop-anhthu.yaml\nMục đích: Định nghĩa toàn bộ tài nguyên AWS dưới dạng code, bao gồm:\n1.1 MWAA Environment (Airflow)\nS3 bucket để lưu DAGs, code (AirflowEnvironmentBucket).\nMWAA Environment (MwaaEnvironment).\nIAM Role cho MWAA (MwaaExecutionRole).\nCác subnet, VPC, security group, NAT gateway, route table, v.v.\n1.2 Redshift Cluster\nRedshift Cluster (RedshiftCluster).\nRedshift Subnet Group, Parameter Group.\nIAM Role cho Redshift (RedshiftIamRole).\nSecrets Manager để lưu thông tin đăng nhập Redshift (RedshiftCreds).\nS3 bucket tạm cho Redshift (RedshiftTempDataBucket).\n1.3 Glue Connection\nKết nối Glue tới Redshift (GlueRedshiftConnection).\nKhông tạo Glue Job.\n1.4 Các thành phần mạng và bảo mật\nVPC, subnet, security group, route table, NAT gateway, VPC endpoint cho S3 Mẫu template này áp dụng chuẩn YAML cho CloudFormation, giúp versioning, review và reuse dễ dàng.\n2. Những chỗ cần chú ý khi tái sử dụng code Trước khi deploy, bạn cần kiểm tra và thay thế các giá trị sau trong phần Parameters hoặc trực tiếp trong template:\nCác tên của các thành phần dịch vụ và các địa chỉ IP, API keys, ARN url, đường dẫn tới các dịch vụ sử dụng, các loại phù hợp. Chọn đúng lớp node (node class) và số lượng node, vùng có giá phù hợp khối lượng dữ liệu.\nVí dụ như: 3. Các bước triển khai (Deployment) Bước 1: Tạo CloudFormation Stack Đăng nhập AWS Console → chọn CloudFormation. Chọn Create stack Chọn Choose an existing template. Chọn Upload a template file, tải lên template-workshop-anhthu.yaml.\nNhấn Next. Điền tên của stack data-workshop-anhthu. Nhấn Next. Chọn I acknowledge that AWS CloudFormation might create IAM resources.\nNhấn Next. Kiểm tra lại kĩ các nội dung và chọn Submit. Nhấn Next qua phần Options (tuỳ chọn thêm Tags). Nhấn Create stack và theo dõi tab Events đến khi trạng thái là CREATE_COMPLETE. Bước 2: Kiểm tra tài nguyên đã tạo S3 IAM Kiểm tra hai IAM Role (MWAA, Glue) với đúng policy. VPC \u0026amp; Security Groups Trong VPC console, xác nhận Subnets và Security Groups cho phép: MWAA truy xuất S3 Glue truy xuất S3 \u0026amp; Redshift Redshift cho phép kết nối port 5439 MWAA Environment Console MWAA hiển thị environment mới, status Available. AWS Glue Xác nhận Connection redshift-demo-connection. Redshift Console Redshift hiển thị cluster available. Tạo bao nhiêu check bấy nhiêu để lúc làm không bị thiếu môi trường hay kết nối (nếu có phải xóa đi tạo stack lại) và có thể note lại để lúc xóa đảm bảo không xóa thiếu dịch vụ tránh tăng các khoản phí không đáng nói.\nBước 3: Kết nối và kiểm thử Redshift "
},
{
	"uri": "//localhost:1313/vi/4-analysis-visualization/4.2-quicksight/",
	"title": "Trực quan hóa với Amazon QuickSight",
	"tags": [],
	"description": "",
	"content": " Amazon QuickSight là dịch vụ BI đám mây, cho phép tạo dashboard tương tác từ nhiều nguồn dữ liệu.\nSử dụng SPICE (Super-fast, Parallel, In-memory Calculation Engine) để đẩy nhanh tốc độ truy vấn và rendering.\nCung cấp các loại biểu đồ đa dạng (line, bar, pie, heatmap, KPI plates…), filter và drill-down.\nKết nối Redshift → QuickSight Tạo Data source\nĐăng nhập QuickSight → chọn Manage data → New data set. Chọn Redshift, nhập: Data source name Cluster: chọn cluster Redshift (hoặc nhập endpoint/manual). Database: tên database. Authentication: sử dụng IAM role hoặc user/password (có lưu trong Secrets Manager). Test connection → Save. Tạo Data set\nChọn schema → table public.weather_data. Chọn Import to SPICE for quicker analytics hoặc Direct query tuỳ nhu cầu. (Tùy chọn) Thêm filter, custom SQL nếu cần. Tạo Analysis và trực quan Khởi tạo Analysis\nTrong QuickSight → Analyses → New analysis → chọn data set vừa tạo. Tạo Visuals\nQuản lý dung lượng SPICE và filter, giảm dataset nếu cần\nXuất Dashboard Chọn Share → Publish dashboard. Đặt tên, phân quyền xem (user, group). Gửi link hoặc embed vào ứng dụng nội bộ. Với QuickSight, chỉ mất vài thao tác để biến bảng weather_data trong Redshift thành các biểu đồ tương tác, hỗ trợ phân tích trực quan và ra quyết định nhanh chóng.\n"
},
{
	"uri": "//localhost:1313/vi/2-architecture/2.2-architecture/",
	"title": "Xây dựng Sơ đồ Kiến trúc",
	"tags": [],
	"description": "",
	"content": "Sơ đồ Kiến trúc Kiến trúc này mô tả một quy trình thu thập, xử lý, lưu trữ, phân tích và trực quan hóa dữ liệu thời tiết thông qua các dịch vụ AWS và công cụ mã nguồn mở. Ban đầu, người dùng gửi yêu cầu thông tin thời tiết. Một Python script được đóng gói và lưu trữ vào môi trường Amazon S3, để Apache Airflow (MWAA) tự động gọi thực thi theo lịch định sẵn. Python script này sử dụng thư viện PySpark (Spark được tích hợp sẵn trong script Python) để gọi API của OpenWeather, lấy dữ liệu thời tiết theo thời gian thực, xử lý sơ bộ và lưu vào bucket Amazon S3 dưới dạng dữ liệu thô. Tiếp theo, AWS Glue Job tiến hành các bước ETL nâng cao, làm sạch, chuẩn hóa dữ liệu và nạp vào Amazon Redshift, sẵn sàng cho việc phân tích chuyên sâu. Sau đó, Amazon QuickSight kết nối với Redshift để tạo ra dashboard tương tác, trực quan hóa dữ liệu giúp người dùng dễ dàng phân tích và ra quyết định.\nToàn bộ hạ tầng và các dịch vụ này được quản lý và triển khai tự động bằng AWS CloudFormation, đồng thời được bảo mật chặt chẽ bởi AWS IAM, VPC và Secrets Manager, giúp kiểm soát truy cập và lưu trữ an toàn các thông tin nhạy cảm trong hệ thống.\nSơ đồ dưới đây minh họa tổng thể kiến trúc hệ thống trên AWS: "
},
{
	"uri": "//localhost:1313/vi/3-etl-process/",
	"title": "Quá trình ETL tự động",
	"tags": [],
	"description": "",
	"content": "Trong kiến trúc thu thập và xử lý dữ liệu thời tiết sử dụng AWS, phần quan trọng nhất nằm ở quy trình ETL (Extract – Transform – Load) tự động. Đây là nơi toàn bộ dữ liệu được lấy về từ OpenWeather API, xử lý, lưu trữ và chuyển hóa thành thông tin có thể phân tích được.\nLuồng xử lý tổng thể gồm 4 bước chính: 1. Chuẩn bị môi trường Viết script Python để gọi API và xử lý dữ liệu ban đầu.\nĐóng gói các file này vào S3 để Airflow (MWAA) có thể chạy tự động.\n2. Triển khai hạ tầng tự động Sử dụng AWS CloudFormation để triển khai toàn bộ tài nguyên cần thiết như S3, MWAA, IAM, Glue, Redshift\u0026hellip;\n3. Cấu hình Airflow Thiết lập biến (Variables) và kết nối (Connections) trong Airflow.\nUpload DAG và các tệp cần thiết lên đúng vị trí trong S3.\n4. Thực thi ETL Apache Airflow tự động kích hoạt pipeline theo lịch trình.\nGọi API → lưu vào S3 → xử lý bằng Glue → lưu vào Redshift → trực quan hóa bằng QuickSight.\nTại sao cần ETL tự động? Tiết kiệm thời gian: Không cần thủ công tải dữ liệu hằng ngày.\nChuẩn hóa dữ liệu: Đảm bảo dữ liệu có định dạng sạch, đồng nhất, có thể phân tích.\nTích hợp dễ dàng: Với các công cụ phân tích như Amazon Redshift \u0026amp; QuickSight.\nCó thể mở rộng: Dễ dàng áp dụng cho nhiều loại dữ liệu khác nhau, không chỉ thời tiết.\nNội dung chính: 3.1 Về các file môi trường\n3.2 Triển khai hạ tầng bằng mã nguồn\n3.3 Thiết lập biến, tạo kết nối và upload môi trường\n3.4 Quá trình thực hiện ETL\n"
},
{
	"uri": "//localhost:1313/vi/3-etl-process/3.3-setup-variables-connection/",
	"title": "Thiết lập biến, tạo kết nối và upload môi trường",
	"tags": [],
	"description": "",
	"content": "3.3 Thiết lập biến, tạo kết nối và upload môi trường Bước 1: Tạo Variables trong Airflow Mở Airflow UI → Admin → Variables → Create.\nTạo biến: Key: key\nValue: \u0026lt;YOUR_OPENWEATHER_API_KEY\u0026gt; Cách lấy API key\nVào trang chủ của OpenWeather API: Chọn (Tên của bạn), vào My API Keys, api key sẽ được mặc định hoặc bạn có thể generate mới: Chắc chắn là API key của bạn đang ở chế độ hoạt động.\nBước 2: Tạo Connections trong Airflow Mở Airflow UI → Admin → Connections → Create. Tạo AWS_CONN (kết nối tới AWS): Conn Id: AWS_CONN Conn Type: Amazon Web Services Login: \u0026lt;AWS_ACCESS_KEY_ID\u0026gt; Password: \u0026lt;AWS_SECRET_ACCESS_KEY\u0026gt; Chú ý cách lấy access key\nLưu ý: Trong DAG transform_redshift_load.py, tham số\naws_conn_id=\u0026#39;AWS_CONN\u0026#39; create_job_kwargs={\u0026#39;Connections\u0026#39;:{\u0026#39;Connections\u0026#39;:[\u0026#39;redshift-demo-connection\u0026#39;]}} Bước 3: Tạo 2 S3 Buckets để chứa dữ liệu raw khi thu thập được và các nội dung trong Glue thực hiện Bước 4: Kết nối db để thực hiện đẩy dữ liệu vào trong Redshift "
},
{
	"uri": "//localhost:1313/vi/4-analysis-visualization/",
	"title": "Phân tích và trực quan hóa",
	"tags": [],
	"description": "",
	"content": "1. Giới thiệu về dữ liệu thu được Sau khi pipeline ETL chạy xong, bạn sẽ có bảng public.weather_data trong Redshift chứa các cột chính sau:\nCột Kiểu dữ liệu Mô tả dt timestamp Thời điểm ghi nhận (chuyển từ epoch) dt_iso timestamp Thời điểm ghi nhận ở định dạng ISO temp double Nhiệt độ trung bình (°C) feels_like double Nhiệt độ cảm nhận (°C) pressure integer Áp suất (hPa) humidity integer Độ ẩm (%) wind_speed double Tốc độ gió (m/s) weather_main varchar Tình trạng thời tiết chính (Clear, Clouds, Rain,…) weather_description varchar Mô tả chi tiết hơn (few clouds, light rain,…) city varchar Tên thành phố country varchar Quốc gia ingest_time timestamp Thời điểm dữ liệu được ETL vào kho (UTC) Dữ liệu được thu thập mỗi giờ, lưu trữ lịch sử liên tục, giúp phân tích xu hướng và biến động thời tiết theo thời gian.\n4.2 Phân tích trên Amazon Redshift Tính toán chỉ số trung bình \u0026amp; biến động Ví dụ: trung bình nhiệt độ hàng ngày, biến động áp suất theo giờ. Phân loại \u0026amp; lọc theo điều kiện Ví dụ: đếm số lần xảy ra “Rain” trong tuần, mức độ ẩm vượt ngưỡng. Xác định xu hướng \u0026amp; seasonality Ví dụ: tăng/giảm nhiệt độ theo mùa, chu kỳ mưa. Kết hợp với dữ liệu kinh doanh Ví dụ: so sánh lưu lượng khách tham quan ngoài trời với điều kiện thời tiết. Redshift với khả năng xử lý OLAP cho phép bạn chạy các truy vấn tổng hợp, phân tích thời gian lớn một cách nhanh chóng và hiệu quả.\n4.3 Trực quan hóa với Amazon QuickSight Dashboard tương tác Dễ dàng kéo thả để tạo biểu đồ line chart (nhiệt độ, độ ẩm), bar chart (số ngày mưa), gauge chart (áp suất trung bình). Filter \u0026amp; drill-down Cho phép chọn khoảng thời gian, thành phố, điều kiện thời tiết để phân tích chi tiết. Alerts \u0026amp; insights tự động Sử dụng tính năng “Anomaly Detection” và “Notifications” để cảnh báo khi nhiệt độ/độ ẩm bất thường. Chia sẻ \u0026amp; nhúng Xuất report, chia sẻ link với đồng nghiệp hoặc nhúng dashboard vào ứng dụng nội bộ. Kết hợp Redshift và QuickSight, bạn có thể triển khai một giải pháp “end-to-end” từ thu thập dữ liệu, phân tích chuyên sâu đến trực quan hóa linh hoạt, hỗ trợ ra quyết định nhanh chóng dựa trên dữ liệu thời tiết.\nNội dung chính: 4.1 Phân tích với AWS Redshift\n4.2 Trực quan hóa với AWS Quicksight\n"
},
{
	"uri": "//localhost:1313/vi/3-etl-process/3.4-etl-execution/",
	"title": "Quá trình thực hiện ETL",
	"tags": [],
	"description": "",
	"content": "Toàn bộ pipeline ETL đã được cấu hình để chạy tự động hàng giờ. Dưới đây là các bước chính khi bạn “go-live”:\nBước 1: Bật (unpause) các DAG trong Airflow Mở Airflow UI → DAGs. Tìm hai DAG openweather_api_dag và transform_redshift_dag. Chuyển toggle từ Off → On để kích hoạt tự động chạy theo schedule_interval (mỗi giờ). Xác nhận cột Schedule hiển thị “@hourly” và status “healthy”. Lưu ý: Bạn cũng có thể bật DAGs qua CLI:\nairflow dags unpause openweather_api_dag airflow dags unpause transform_redshift_dag Bước 2: Theo dõi, đợi và kiểm tra (nếu lỗi) Theo dõi lần chạy đầu\nVào tab Graph hoặc Tree view của openweather_api_dag. Quan sát Task extract_api_data → upload_to_S3. Chờ trạng thái từng task chuyển thành success (màu xanh lá). Kiểm tra dữ liệu thô trên S3\nVào S3 Console → bucket raw-data. Xác nhận file raw/weather_api_data.csv đã được viết mới (timestamp gần nhất). Chờ DAG ETL tiếp theo\nTương tự với transform_redshift_dag, quan sát task wait_openweather_api → transform_task. Khi thành công, Glue Job sẽ chạy và nạp dữ liệu vào Redshift. Kiểm tra dữ liệu trong Redshift\nVào Query Editor → chạy: SELECT COUNT(*) FROM public.weather_data; Xác nhận kết quả (≥ 0) và dữ liệu mới nhất có ingest_time phù hợp. Xử lý lỗi (nếu có)\nNếu task bất kỳ fail, click vào task → Logs để xem chi tiết. Thông thường lỗi có thể do: API key không hợp lệ (Task extract_api_data). Bucket S3 sai đường dẫn hoặc quyền truy cập. Glue Job script lỗi khi parse hoặc connect Redshift. IAM Role thiếu policy. Sửa code hoặc cấu hình, commit \u0026amp; sync lại DAGs/scripts, then clear task → rerun. Riêng mình gặp phải lỗi Có thể thêm Giám sát dài hạn nếu lấy dữ liệu lâu. Ex: Dùng CloudWatch \u0026amp; Cost Explorer để theo dõi logs và chi phí.\nTóm lại, chỉ cần bật DAGs, sau đó Airflow tự động chạy, theo dõi kết quả và can thiệp khi có lỗi để đảm bảo pipeline vận hành ổn định và dữ liệu luôn được cập nhật.\n"
},
{
	"uri": "//localhost:1313/vi/5-cleanup/",
	"title": "Dọn dẹp tài nguyên",
	"tags": [],
	"description": "",
	"content": "Mục đích xóa Dọn dẹp tài nguyên không còn cần thiết, tránh nhầm lẫn và rối rắm trong quản lý. Ngăn ngừa chi phí phát sinh: loại bỏ các dịch vụ đang chạy hoặc lưu trữ dữ liệu không dùng. Bảo mật: giảm thiểu rủi ro lộ thông tin và bề mặt tấn công. Tiết kiệm chi phí: AWS tính phí theo thời gian sử dụng và dung lượng. Xóa kịp thời giúp giảm đáng kể hoá đơn. Quản lý gọn gàng: Giúp môi trường AWS sạch sẽ, minh bạch, dễ theo dõi và bảo trì. 5.1 Dừng và xóa DAGs trong Airflow Truy cập Airflow UI → trang DAGs Chuyển hai DAG openweather_api_dag và transform_redshift_dag về trạng thái Off Vào S3 Console → bucket chứa DAGs → xóa các file DAG tương ứng 5.2 Xóa CloudFormation Stack Truy cập AWS Console → dịch vụ CloudFormation Chọn stack đã tạo (ví dụ anhthu-etl-stack) Chọn Delete stack và xác nhận Đợi trạng thái chuyển thành DELETE_COMPLETE 5.3 Xóa hai S3 Buckets Truy cập S3 Console → chọn bucket DAGs và bucket dữ liệu thô Mở từng bucket, chọn Empty bucket để xóa toàn bộ nội dung Sau khi trống, chọn Delete bucket và xác nhận 5.4 Kiểm tra lại Xác nhận không còn stack nào trong CloudFormation Kiểm tra S3 Console đảm bảo hai bucket đã bị xóa Vào Airflow UI đảm bảo không còn DAG hiển thị Vào Redshift Console đảm bảo cluster do CF tạo (nếu có) đã bị xóa hoặc không còn Check kĩ lại các security group để đảm bảo xóa không thiếu bất cứ dịch vụ nào. "
},
{
	"uri": "//localhost:1313/vi/6-in-conclusion/",
	"title": "Kết luận",
	"tags": [],
	"description": "",
	"content": "Lab: Xây dựng kiến trúc xử lý tự động và trực quan hóa dữ liệu thời gian thực trên AWS Chúng ta đã hoàn thành một hành trình “end-to-end” từ:\nViết Python script thu thập dữ liệu thời tiết từ OpenWeather API. Điều phối workflow với Apache Airflow (MWAA), tận dụng XCom để truyền dữ liệu giữa các task. Triển khai toàn bộ hạ tầng bằng AWS CloudFormation: S3, IAM, VPC, MWAA, Glue, Redshift, Secrets Manager. Xử lý và chuyển đổi dữ liệu bằng AWS Glue (PySpark), nạp vào Amazon Redshift với tối ưu DIST/SORT keys. Phân tích dữ liệu lịch sử và realtime bằng SQL trên Redshift. Trực quan hóa và chia sẻ insight tức thì qua Amazon QuickSight. Những gì bạn đã học được: Thiết kế kiến trúc ETL tự động, tối ưu cho dữ liệu thời gian thực. Áp dụng IaC với CloudFormation để duy trì reproducibility và version control. Tích hợp dịch vụ AWS linh hoạt: từ storage (S3), compute (Glue), data warehouse (Redshift) đến BI (QuickSight). Tối ưu performance \u0026amp; chi phí: chọn node phù hợp, sử dụng materialized views, auto-pause, Reserved Instances. Xử lý lỗi và giám sát: sensor, retry, alert trong Airflow; CloudWatch và Cost Explorer. Hướng phát triển tương lai: Mở rộng nguồn dữ liệu: tích hợp IoT, logs, streams (Kinesis, Kafka). Machine Learning: dùng SageMaker để dự báo thời tiết, anomaly detection. CI/CD cho hạ tầng: tự động test \u0026amp; deploy DAGs, template CloudFormation. Lakehouse \u0026amp; Data Mesh: kết hợp Athena, Glue Elastic Views, Redshift Serverless để giảm độ trễ và chi phí. Cảm ơn mọi người đã làm theo workshop “Xây dựng kiến trúc xử lý tự động và trực quan hóa dữ liệu thời gian thực trên AWS” của mình. Chúc các bạn thành công và tiếp tục khám phá những giải pháp dữ liệu mạnh mẽ trên AWS!\nEm xin gửi lời cảm ơn chân thành nhất đến mọi người đã luôn đồng hành và hỗ trợ em suốt 3 tháng “lên mây” đầy thử thách này! 🚀💻\r"
},
{
	"uri": "//localhost:1313/vi/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "//localhost:1313/vi/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]